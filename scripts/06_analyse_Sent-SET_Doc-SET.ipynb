{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f31e90",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '3.12.1 (Python 3.12.1)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/home/asusaiyah/.pyenv/versions/3.12.1/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dataset = \"P-SET\"\n",
    "llm_type = \"llama3.1:70b\"\n",
    "prompt_version=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd235189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrices_for_column_pairs(df, gt_col,vis_columns):\n",
    "\n",
    "    n_pairs = len(vis_columns)\n",
    "\n",
    "    # Determine layout: square-ish grid\n",
    "    n_cols = int(np.ceil(np.sqrt(n_pairs)))\n",
    "    n_rows = int(np.ceil(n_pairs / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    col1 = gt_col\n",
    "    for idx, col2 in enumerate(vis_columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Drop rows with missing values in either column\n",
    "        sub_df = df[[col1, col2]].dropna().copy()\n",
    "        start_length = len(sub_df)\n",
    "        # sub_df = sub_df[sub_df.apply(lambda x: True if (\"_\" not in x[col1] and '_' not in x[col2]) else False, axis=1)]\n",
    "        filter_length = len(sub_df)\n",
    "        # Get confusion matrix\n",
    "        labels = sorted(set(sub_df[col1]) | set(sub_df[col2]))\n",
    "        cm = confusion_matrix(sub_df[col1], sub_df[col2], labels=labels)\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "        ax.set_title(f\"{col1} vs \\n{col2}\\n strt: {start_length}\\nelim.:{filter_length-start_length}\", fontsize=10)\n",
    "        ax.set_xlabel(col2)\n",
    "        ax.set_ylabel(col1)\n",
    "\n",
    "    # Hide any extra axes\n",
    "    for j in range(len(vis_columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cfcadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'most_common_unigram': [('orange', 3),\n",
       "  ('apple', 2),\n",
       "  ('ate', 2),\n",
       "  ('babana', 2)],\n",
       " 'most_common_bigram': [(('apple', 'ate'), 2),\n",
       "  (('orange', 'babana'), 2),\n",
       "  (('ate', 'orange'), 1)],\n",
       " 'most_common_trigram': [(('apple', 'ate', 'orange'), 1)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Download tokenizer models if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def most_frequent_ngrams(sentences, N=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        # Remove stopwords and punctuation\n",
    "        filtered_tokens = [t for t in tokens if t not in stop_words and t not in punctuation]\n",
    "\n",
    "        all_unigrams.extend(filtered_tokens)\n",
    "        all_bigrams.extend(ngrams(filtered_tokens, 2))\n",
    "        all_trigrams.extend(ngrams(filtered_tokens, 3))\n",
    "\n",
    "    # Count frequencies\n",
    "    unigram_counts = Counter(all_unigrams)\n",
    "    bigram_counts = Counter(all_bigrams)\n",
    "    trigram_counts = Counter(all_trigrams)\n",
    "\n",
    "    # Get top N most common\n",
    "    most_common_unigram = unigram_counts.most_common(N)\n",
    "    most_common_bigram = bigram_counts.most_common(N)\n",
    "    most_common_trigram = trigram_counts.most_common(N)\n",
    "\n",
    "    return {\n",
    "        'most_common_unigram': most_common_unigram,\n",
    "        'most_common_bigram': most_common_bigram,\n",
    "        'most_common_trigram': most_common_trigram\n",
    "    }\n",
    "\n",
    "\n",
    "most_frequent_ngrams([\"apple ate orange\", \"orange at babana\", \"apple ate\", \"orange at babana to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Sleep************************\n",
      "['../exports/05_llm_llama3.1:70b_P-SET_19sep/Sleep/Sleep_Documents_att_True.pkl', '../exports/05_llm_llama3.1:70b_P-SET_19sep/Sleep/Sleep_Sentences_att_True.pkl']\n",
      "Sleep_Documents_att_True\n",
      "Ao True Sleep_Documents_att_True\n",
      "****************Document**************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_keyword_present\n",
       "0.0    57\n",
       "1.0    42\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc_gt_Sleep  is_keyword_present\n",
      "False         0.0                   57\n",
      "True          1.0                   39\n",
      "False         1.0                    3\n",
      "Name: count, dtype: int64\n",
      "Event_Name_Sleep\n",
      "Event_Name_LLM_Events_all_evidence_Doc_Sleep\n",
      "false_positive_keywords: {'most_common_unigram': [('patient', 12), ('overnight', 7), ('lightly', 5), ('sedated', 5), ('well', 4), ('rate', 4), ('x', 3), ('oob', 3), ('tolerated', 3), ('x3', 3), ('drops', 3), ('changes', 3), ('restlessness', 2), ('slept', 2), ('mae', 2)], 'most_common_bigram': [(('lightly', 'sedated'), 4), (('patient', 'lightly'), 3), (('oriented', 'x3'), 2), (('sedated', 'fentanyl'), 2), (('mcg/heart', 'rate'), 2), (('rate', 'versed'), 2), (('5', 'mg/heart'), 2), (('mg/heart', 'rate'), 2), (('changes', 'made'), 2), (('made', 'overnight'), 2), (('sedated', 'propofol'), 2), (('patient', 'restless'), 1), (('periods', 'confusion'), 1), (('dangling', 'bedside'), 1), (('bedside', 'times'), 1)], 'most_common_trigram': [(('patient', 'lightly', 'sedated'), 3), (('lightly', 'sedated', 'fentanyl'), 2), (('mcg/heart', 'rate', 'versed'), 2), (('5', 'mg/heart', 'rate'), 2), (('changes', 'made', 'overnight'), 2), (('dangling', 'bedside', 'times'), 1), (('bedside', 'times', 'c/o'), 1), (('times', 'c/o', 'restlessness'), 1), (('c/o', 'restlessness', 'baseline'), 1), ((\"n't\", 'slept', 'days'), 1), (('x', '3', 'mae'), 1), (('3', 'mae', 'oob'), 1), (('mae', 'oob', 'commode'), 1), (('oob', 'commode', 'x'), 1), (('commode', 'x', '1'), 1)]}\n",
      "TP 37 TN 21 FP 39 FN 2 %%%%%%%%%%%%%%%%%%%%%%%\n",
      "Event_Name_LLM_Events_keyword_evidence_Doc_Sleep\n",
      "file written to ../exports/06_analysis/Sleep/all_correct_Sleep_Documents_att_True.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>technique</th>\n",
       "      <th>pos_sup</th>\n",
       "      <th>neg_sup</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Event_Name_Sleep</td>\n",
       "      <td>Sleep_Ao</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.000</td>\n",
       "      <td>39</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Event_Name_LLM_Events_all_evidence_Doc_Sleep</td>\n",
       "      <td>LLM_KiEi_Doc_Ao</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.949</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>14.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Event_Name_LLM_Events_keyword_evidence_Doc_Sleep</td>\n",
       "      <td>LLM__Ki_Doc_Ao</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.949</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>14.782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           col_name        technique  pos_sup  \\\n",
       "0                                  Event_Name_Sleep         Sleep_Ao       39   \n",
       "1      Event_Name_LLM_Events_all_evidence_Doc_Sleep  LLM_KiEi_Doc_Ao       39   \n",
       "2  Event_Name_LLM_Events_keyword_evidence_Doc_Sleep   LLM__Ki_Doc_Ao       39   \n",
       "\n",
       "   neg_sup  f1_score  precision  recall  TP  TN  FP  FN    time  \n",
       "0       60     0.963      0.929   1.000  39  57   3   0   0.003  \n",
       "1       60     0.643      0.487   0.949  37  21  39   2  14.864  \n",
       "2       60     0.632      0.474   0.949  37  19  41   2  14.782  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep_Sentences_att_True\n",
      "Ao True Sleep_Sentences_att_True\n",
      "****************Sentence**************\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from itertools import product\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "\n",
    "disagreement_type = \"correct\"\n",
    "llm_type = \"llama3.1:70b\"\n",
    "suffix = \"19sep\"\n",
    "def get_time(x):\n",
    "    x = np.array(x)\n",
    "    # Remove NaNs\n",
    "    x = x[~np.isnan(x)]\n",
    "    # If less than 3 values, just return mean\n",
    "    if len(x) < 3:\n",
    "        return np.mean(x)\n",
    "    # Remove outliers using IQR\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    mask = (x >= q1 - 1.5 * iqr) & (x <= q3 + 1.5 * iqr)\n",
    "    return np.mean(x[mask])\n",
    "\n",
    "def fixnames(ls,suffix,remove):\n",
    "    LS=[]\n",
    "    for s in ls:\n",
    "        s = s.replace(\"_keyword_evidence\",\"_Ki\").replace(\"_example_evidence\",\"_Ei\").replace(\"_no_evidence\",\"\").replace(\"_all_evidence\",\"KiEi\").replace(\"Event_Name_\",'').replace(\"_Events\",'_').replace('_'+remove,\"\")\n",
    "        s = s + \"_\" + suffix\n",
    "        if \"sentence\" in s:\n",
    "            s = s.replace(\"sentence\",\"\")\n",
    "            s = s + \"_Sent-SET\"\n",
    "        elif \"document\" in s:\n",
    "            s = s.replace(\"document\",\"\")\n",
    "            s = s + \"_Doc-SET\"\n",
    "        LS.append(s)\n",
    "    return LS\n",
    "\n",
    "def get_col_suffix(keyword_input, example_input):\n",
    "    col_suffix = \"no\"\n",
    "    if keyword_input and example_input:\n",
    "        col_suffix = \"all\"\n",
    "    elif keyword_input and not example_input:\n",
    "        col_suffix = \"keyword\"\n",
    "    elif not keyword_input and example_input:\n",
    "        col_suffix = \"example\"\n",
    "    return col_suffix\n",
    "\n",
    "LLM_dict = {}\n",
    "llm_models_all = []\n",
    "for keyword_input, example_input in [i for i in product([True,False],[True,False])]: \n",
    "    for input_type in [\"Sent\", \"Doc\"]:\n",
    "        col_suffix = get_col_suffix(keyword_input, example_input)\n",
    "        llm_models_all.append(f\"LLM_Events_{col_suffix}_evidence_{input_type}\")\n",
    "df_both = pd.read_pickle(\"../exports/04_dictionary_features.pkl\")\n",
    "\n",
    "\n",
    "def ispresent(a,b):\n",
    "    res = []\n",
    "    for i in a:\n",
    "        presence = False\n",
    "        for j in b:\n",
    "            if (i in j) or (j in i):\n",
    "                presence = True\n",
    "                break\n",
    "        res.append(presence)\n",
    "    return res\n",
    "\n",
    "dataset = \"P-SET\"\n",
    "for analysis_type in [\"all\"]: #(M-SET, A-SET, D-SET, F-SET)\n",
    "    for ET in [\"Sleep\"]:\n",
    "        print(f\"********************{ET}************************\")\n",
    "        llm_files = glob(f\"../exports/05_llm_{llm_type}_{dataset}_{suffix}/{ET}/*.pkl\")\n",
    "        print(llm_files)\n",
    "        try:\n",
    "            gt_file_sent = glob(f\"../exports/04_groundtruth/{dataset}/Annotated/{ET}_Sentences.pkl\")[0]\n",
    "            gt_file_doc = glob(f\"../exports/04_groundtruth/{dataset}/Annotated/{ET}_Documents.pkl\")[0]\n",
    "        except IndexError:\n",
    "            print(f\"No ground truth file found for {ET} in ../exports/04_groundtruth/{dataset}/Annotated/\")\n",
    "            continue    \n",
    "        \n",
    "        llm_files = [i for i in llm_files if \"True\" in i]\n",
    "        # llm_files = [i for i in llm_files if \"Sent\" in i]\n",
    "        for file in llm_files:\n",
    "            filename = os.path.basename(file).rstrip('.pkl')\n",
    "            print(filename)\n",
    "            text_type = \"Document\" if \"Document\" in filename else \"Sentence\"\n",
    "            _,attribute_requested = filename.split(\"_\")[-2:]\n",
    "            attribute_requested = eval(attribute_requested)\n",
    "            suffix = \"Ao\" if attribute_requested else \"\"           \n",
    "            print(suffix,attribute_requested,filename)\n",
    "            \n",
    "            if text_type == \"Sentence\":\n",
    "                gt_file = gt_file_sent\n",
    "                id_type = \"UID\"\n",
    "                focus = \"Sent\"\n",
    "                gt_column = f\"Sent_gt_{ET}\"\n",
    "            elif text_type == \"Document\":\n",
    "                gt_file = gt_file_doc\n",
    "                id_type = \"ROW_ID\"\n",
    "                focus = \"Doc\"\n",
    "                gt_column = f\"Doc_gt_{ET}\"\n",
    "            print(f\"****************{text_type}**************\")\n",
    "            gt_df = pd.read_pickle(gt_file)\n",
    "            gt_df[id_type] = gt_df[id_type].astype(str)\n",
    "            df_both[id_type] = df_both[id_type].astype(str)\n",
    "            gt_df = gt_df.dropna(subset=gt_column)\n",
    "            if not attribute_requested:\n",
    "                gt_df[gt_column] = gt_df.apply(lambda x: x[gt_column] if x['negation']==False else False,axis=1)\n",
    "            \n",
    "            gt_df['Lemma'] = gt_df['Lemma'].apply(lambda x: tuple(x))\n",
    "            gt_df[\"is_keyword_present\"] = gt_df[\"Event_Name\"].apply(lambda x: 1 if ET in x else 0)\n",
    "            \n",
    "            id_to_gt = {row[id_type]:row[gt_column] for _,row in gt_df.iterrows()}\n",
    "            id_to_negation = {row[id_type]:row['negation'] for _,row in gt_df.iterrows()}\n",
    "            id_to_key_present = {row[id_type]:row[f\"is_keyword_present\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_lemma = {row[id_type]:row[f\"Lemma\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_comment = {row[id_type]:row[f\"comment\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_dict_time = {row[id_type]:row[f\"Time\"] for _,row in df_both.iterrows()}\n",
    "            \n",
    "            output_folder = f\"../exports/06_analysis/{ET}\"\n",
    "            os.makedirs(output_folder,exist_ok=True)\n",
    "       \n",
    "            \n",
    "            \n",
    "            df = pd.read_pickle(file)\n",
    "            df[\"num_events_keyword\"] = df[\"Event_Name\"].apply(lambda x: len(x))\n",
    "            df[f\"num_{ET}_keyword\"] = df[\"Event_Name\"].apply(lambda x: len([i for i in x if i == ET]))\n",
    "            df[id_type] = df[id_type].astype(str)\n",
    "            llm_models = [i for i in llm_models_all if i in df]\n",
    "            df[f'{ET}_time'] = df[id_type].apply(lambda x:id_to_dict_time.get(x))\n",
    "            df[\"focus_event\"] = ET\n",
    "            df['negation'] = df[id_type].apply(lambda x:id_to_negation.get(x))\n",
    "            df['comment'] = df[id_type].apply(lambda x:id_to_comment.get(x))\n",
    "            \n",
    "            \n",
    "            df[gt_column] = df[id_type].apply(lambda x:id_to_gt.get(x,None))\n",
    "            df[f\"is_keyword_present\"] = df[id_type].apply(lambda x:id_to_key_present.get(x))\n",
    "            display(df.is_keyword_present.value_counts())\n",
    "            print(df[[gt_column,\"is_keyword_present\"]].value_counts())\n",
    "                        \n",
    "            df[f\"dict_Lemma\"] = df[id_type].apply(lambda x:id_to_lemma.get(x))\n",
    "            df.dropna(subset=gt_column,inplace=True)\n",
    "            df[gt_column] = df[gt_column].astype(int)\n",
    "            \n",
    "            \n",
    "\n",
    "            for col in llm_models:\n",
    "                df[f\"{col}_{ET}_time\"] = df[col].apply(lambda x:x['event_detection_time'])\n",
    "                \n",
    "            if len(df) > 1:\n",
    "                first_row = df.iloc[0]\n",
    "                splittable_columns = [\"Event_Name\"]+[f\"Event_Name_{model}\" for model in llm_models]\n",
    "                disagreement_dfs = []\n",
    "                generated_columns = []\n",
    "\n",
    "                for col in splittable_columns:\n",
    "                    generated_column = f\"{col}_{ET}\"\n",
    "                    df[generated_column] = df[col].apply(lambda x: 1 if ET in x else 0)\n",
    "                    \n",
    "                    generated_columns.append(generated_column)\n",
    "                \n",
    "                f1s, accs, precs, recs, psup, nsup, times, tp, tn, fp, fn = [],[],[],[],[],[],[],[],[],[],[]\n",
    "                \n",
    "                \n",
    "                for col in generated_columns:\n",
    "                    print(col)\n",
    "                    df_temp = df.copy()\n",
    "                    y_gt = df_temp[gt_column]\n",
    "                    LLM_dict[(attribute_requested,gt_column)] = y_gt\n",
    "                    preds = df_temp[col]\n",
    "                    LLM_dict[(attribute_requested,col)] = preds\n",
    "                    f1s.append(f1_score(y_gt, preds))\n",
    "                    accs.append(accuracy_score(y_gt, preds))\n",
    "                    precs.append(precision_score(y_gt, preds, zero_division=0))\n",
    "                    recs.append(recall_score(y_gt, preds))   \n",
    "                    psup.append(sum(y_gt))\n",
    "                    nsup.append(sum(y_gt==0))\n",
    "                    times.append(get_time(df_temp[f\"{col.lstrip('Event_Name_')}_time\"]))\n",
    "                    tn_i, fp_i, fn_i, tp_i = confusion_matrix(y_gt, preds).ravel()\n",
    "                    tp.append(tp_i)\n",
    "                    tn.append(tn_i)\n",
    "                    fp.append(fp_i)\n",
    "                    fn.append(fn_i)\n",
    "                    \n",
    "                    \n",
    "                    if \"all_evidence\" in col:\n",
    "                        col_base = col.lstrip(\"Event_Name_\").rstrip(f\"_{ET}\")\n",
    "                        df[f\"Text_Quote_{col_base}_{ET}\"] = df.apply(lambda x:[t for t,e in zip(x[f\"Text_Quote_{col_base}\"],x[f\"Event_Name_{col_base}\"]) if e==ET],axis=1)\n",
    "                        df[f\"Attribute_{col_base}_{ET}\"] = df.apply(lambda x:[a for a in (x[f\"Attribute_{col_base}\"]) if list(a.keys())[0]==ET],axis=1)\n",
    "                        df[f\"Event_Id_{col_base}\"] = df[col_base].apply(lambda x:[ f\"{eid}={en}|{1 if tq in x['text'] else 0}:{tq}\" for (eid,en,tq) in zip(x['event_id'],x['event'],x['text_quotes'])])\n",
    "                        df[f\"Text_Quote_in_gt\"] = df.apply(lambda x:ispresent(x[f\"Text_Quote_{col_base}_{ET}\"], x['comment'].split('|')), axis=1)\n",
    "\n",
    "                        interesting_columns = [id_type, text_type, gt_column, \n",
    "                                               \"is_keyword_present\",\n",
    "                                               f\"Event_Name_{ET}\", \"comment\", \"negation\", \n",
    "                                               col, \"Text_Quote_in_gt\",\n",
    "                                               f\"Text_Quote_{col_base}_{ET}\",\n",
    "                                               f\"Attribute_{col_base}_{ET}\"]+[i+col_base for i in [\"Event_Name_\",\"Attribute_\",\"Text_Quote_\", \"Event_Id_\",\"Order_\"]] \n",
    "                        \n",
    "                        \n",
    "                        false_positive_samples = df[(df[col]!=df[gt_column])&(df[col]==1)]\n",
    "                        false_positive_samples[interesting_columns].to_excel(f\"../exports/06_analysis/{ET}/FP_{col}.xlsx\")\n",
    "                        print(\"false_positive_keywords:\",most_frequent_ngrams(flatten_list_of_lists(false_positive_samples[f\"Text_Quote_{col_base}_{ET}\"].to_list()),15))\n",
    "                        false_negative_samples = df[(df[col]!=df[gt_column])&(df[col]==0)]\n",
    "                        false_negative_samples[interesting_columns].to_excel(f\"../exports/06_analysis/{ET}/FN_{col}.xlsx\")\n",
    "                        # print(\"false_negative_keywords:\",most_frequent_ngrams(flatten_list_of_lists(false_positive_samples[f\"comment\"].to_list()),10))\n",
    "                        true_positive_samples = df[(df[col]==df[gt_column])&(df[gt_column]==1)]\n",
    "                        true_positive_samples[interesting_columns].to_excel(f\"../exports/06_analysis/{ET}/TP_{col}.xlsx\")\n",
    "                        true_negative_samples = df[(df[col]==df[gt_column])&(df[gt_column]==0)]\n",
    "                        true_negative_samples[interesting_columns].to_excel(f\"../exports/06_analysis/{ET}/TN_{col}.xlsx\")\n",
    "                        print(\"TP\",len(true_positive_samples),\"TN\",len(true_negative_samples),\"FP\",len(false_positive_samples),\"FN\",len(false_negative_samples),\"%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "           \n",
    "                results_df = {\"col_name\":generated_columns, \"technique\":fixnames(generated_columns,suffix,ET), \"pos_sup\": psup, \"neg_sup\": nsup, \"f1_score\":f1s, \"precision\":precs, \"recall\":recs, \"TP\":tp, 'TN':tn, 'FP':fp, 'FN':fn, \"time\":times} \n",
    "                results = pd.DataFrame(results_df)   \n",
    "                plot_confusion_matrices_for_column_pairs(df, gt_column,vis_columns=generated_columns)\n",
    "                op_path = f\"{output_folder}/{analysis_type}_{disagreement_type}_{filename}.xlsx\"\n",
    "                df.to_excel(op_path,index=False)\n",
    "                print(f\"file written to {op_path}\")\n",
    "                display(results)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a036f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sister remained at bedside, sleeping on cot',\n",
       " 'patient placed on 4l nc to sleep',\n",
       " 'eval sleep apnea',\n",
       " '[**name (ni) **] husband will sleep here tonight',\n",
       " 's: \"my son doesn\\'t help at all at home, all he does is eat and sleep\"',\n",
       " 'monitor resp sts/sleep apnea',\n",
       " 'patient reported history of sleep apnea and uses oral airway piece at hs at home',\n",
       " 'plan: keep diastolic blood pressure 100-160 and wean nipride as tolerated female/u with sleep apnea issue with team in am-have wife bring in oral airway piece neuro checks vent drain @5 above the tragus notify h.o. with any changes',\n",
       " 'wife states patient wears oral airway piece r/t sleep apnea which she will bring in during next visit (sat)',\n",
       " 'plan: cont to wean nipride as tol, female/u w/sleep apnea when wife brings in oral device, nuero check q1hr and pro re nata, vent drain now 10 above tragus, notify team w/any changes',\n",
       " 'oral airway piece at bedside for sleep apnea',\n",
       " 'plan: cont to monitor vs to [**last name (un) 2284**] diastolic blood pressure <160, mouthpiece at bedside for sleep apnea, cont neuro checks, monitor vent drain now 20 above tragus, notify team w/any changes',\n",
       " 'he has sleep apnea and when asleep his sat will drop to the high 90s and then comes up to the mid 90s again',\n",
       " 'ms- ambien for sleep- very much awake on/off all of night',\n",
       " ',daughter in room sleeping overnt-',\n",
       " 'sedation/sleep meds',\n",
       " 'however when he sleeps he will occasionally drops sats to high 80s, but will [**doctor last name 419**] back up to high 90s when awakened',\n",
       " 'explain to patient about sleep apnea and how patient saturation dropped will affect the heart/body',\n",
       " 'states that he has sleep apnea but is unable to wear cpap mask at home d/t uncomfortable',\n",
       " 'teach patient about need to have adequate sleeping -->notify dr',\n",
       " 'robitussin with codiene ordered for hs, to aide sleep, otherwise they prefer he clear secretions during day',\n",
       " 'asking for sleeping aid again',\n",
       " \"in crease captopril dose while sleeping bp in the 80's .. goood response from [** **]\",\n",
       " 'gave 1mg by mouth ativan for sleep',\n",
       " 'ccu nursing progress note 1900-0700: 3vd, preop cabgs-\" am i getting a sleeping pill tonight',\n",
       " 'ativan ordered for sleep per anesthesia',\n",
       " 'stated that he was very tired and wanted to sleep around 9pm',\n",
       " 'i discussed the issue of bipap with him and that if he dropped his sa02 while asleep, we would be placed on bipap',\n",
       " 'i stated that he would have to undergo a sleep study for an official diagnosis.patient',\n",
       " 'monitor sao2 while asleep',\n",
       " 'will need bipap when sleeping',\n",
       " 'spo2 on 6l nc ranged from 91% when asleep to 96 after c&db, i.s',\n",
       " 'requesting ativan for sleep',\n",
       " 'reluctant to give sleep aid due to psychosis and hallucinations to benzos this admission',\n",
       " 'although when ep fellow was testing his bundle resulted in a 4.5 second pause until junc escape.resp-ls clear with oxygen saturation 96-97% on 2-3lnp',\n",
       " 'had periods of confusion whereby he thought there were people in his room and that there was someone sleeping in the bed with him',\n",
       " 'social: parents and sister in most of night, parents slept in micu waiting room',\n",
       " 'requested something for sleep adn received ativan 2mg by mouth but patient awaoke at 3am disoriented to place and was removing ekg leads',\n",
       " 'mum and dad at bedside and sleeping in waiting area',\n",
       " 'to facilitate sleep & help control bp',\n",
       " 'patient requesting sleep aid/ \"pill to relax\"',\n",
       " 'wife called during shift, expressing concern over amount of sleep',\n",
       " 'wear cpap at night for sleep apnea',\n",
       " 'social=wife present throughout shift-slept in room',\n",
       " 'ccu npn 2300-0700s:\"i just want to go to sleep',\n",
       " 'o:neuro=alert/awake/oriented',\n",
       " 'rr 28 sats 98%',\n",
       " 'propofol off ~0030.patient quite restless at that time with decreasing mvo2 sat->propofol restarted',\n",
       " 'improving once awake',\n",
       " 'patient intubated at 12:40 and sedated with fentanyl, versed',\n",
       " 'neuro: patient alert oriented x3, denies pain',\n",
       " 'daughter with patient in room all night',\n",
       " 'but for when woken up for questions',\n",
       " 'pulm:  4l/nc, good sats, good cough',\n",
       " \"bs's decreased, ess clear\",\n",
       " 'groggy-received versed & fent in cath lab',\n",
       " 'on rm air 7.44/29/67/20.c/r bloody ,spec sent .sat 98 0n 2lnp',\n",
       " 'appears comfortable this eve/nite.alert/oriented-',\n",
       " 'neuro: alert + oriented x 3, pleasant + cooperative',\n",
       " 'cv- tele sb/sr, no vea, heart rate 58-72, conts with labile bps 86-140s',\n",
       " 'on at 4l. sat.100%',\n",
       " 'this am with assist-up for 1 1/2hrs tol well',\n",
       " 'in & out of bed as per patient',\n",
       " 'k+3.8 repleted with 20meq kcl iv at 3am, am labs do not reflect repletion',\n",
       " 'resp: tol cpap',\n",
       " 'patient remains on bedrest this shift',\n",
       " 'husband stayed overnight',\n",
       " 'cpap+ps 10+peep 10, slight resp alk of 7.46->md aware and no orders given',\n",
       " 'resp: ls clear with dim bases bil. patient placed on mask ventitlation ~ 2-3 hrs overnight',\n",
       " 'check ptt at 5am',\n",
       " 'patient placed on cpap + ps',\n",
       " 's: \"i am sweating so much tonight',\n",
       " 'ccu nsg progress note 7p-7a/ status post aortic dissection',\n",
       " 'bedrest until 9pm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive_samples[text_type].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06630c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f\"Text_Quote_in_gt\"] = df.apply(lambda x:[((i in j) or (j in i)) for (i,j) in product(x[f\"Text_Quote_{col_base}_{ET}\"], x['comment'])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c910cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_samples[interesting_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecffd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[col]==df[gt_column])&(df[col]==1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{col}_{ET}\", df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[f\"{col.lstrip('Event_Name_')}_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "afile=\"../exports/06_analysis/Sleep/all_correct_Sleep_Sentences_att_True.xlsx\"\n",
    "amodel = \"Event_Name_LLM_Events_all_evidence_Sent_Sleep\"\n",
    "agt = \"Sent_gt_Sleep\"\n",
    "\n",
    "adf = pd.read_excel(afile)\n",
    "adf[adf[amodel]!=adf[agt]][['Sentence',agt,amodel]].to_excel(f\"../exports/temp_false_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_gt = {row[id_type]:row[gt_column] for _,row in gt_df.iterrows()}\n",
    "set(id_to_gt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ea880",
   "metadata": {},
   "outputs": [],
   "source": [
    "row['Sent_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{output_folder}/{analysis_type}_{disagreement_type}_{filename}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df_temp[(df_temp[gt_column]==False) & (df_temp[col]==True) & (df_temp[\"Event_Name_Sleep\"]==False)][[id_type,text_type, gt_column, col]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(file)\n",
    "llm_models = [i for i in llm_models_all if i in df]\n",
    "file,llm_models, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b05b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_type =\"Document\" \n",
    "file = f'../exports/05_llm_llama3.1:70b_P-SET/Sleep/Sleep_{text_type}s_att_True.pkl'\n",
    "llm_df = pd.read_pickle(file)\n",
    "for llm_model in llm_models:\n",
    "    llm_df['tqvalid']=llm_df.apply(lambda x:[i in x[text_type] for i in x[f\"Text_Quotes_{llm_model}\"]], axis=1)\n",
    "    print(llm_model, llm_df['tqvalid'].explode().value_counts())\n",
    "# Text_Quotes_LLM_Events_all_evidence_Sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9184bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"DOCUMENT\",\"Event_Name_LLM_Events_keyword_evidence_document_Ao\",\"Sentence\" ,\"GT_Sleep\"]].to_excel(\"../exports/temp_result.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38140d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'dictionary_Sleep_time' in df.columns, llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3aff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac5c04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d947a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.Keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86164cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary = pd.read_pickle(f\"../exports/03_selected_reports_with_event_log_only_dictionary_v2/combined.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary['Events'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "ET = \"Sleep\"\n",
    "df = pd.read_pickle(glob(f\"../exports/04_groundtruth/P-SET/Generated/{ET}*.pkl\")[0])\n",
    "print(df.UID.nunique(), df.Sentence.nunique(), df.ROW_ID.nunique(), df.SUBJECT_ID.nunique(), df.Event_Name.value_counts())\n",
    "\n",
    "df_exploded = df.explode(['Keyword','Lemma','Event_Name','Keyword_Position'])\n",
    "df_exploded['KUID'] = df_exploded['UID'] + \"_\" + df_exploded['Keyword_Position'].astype(str)\n",
    "print(df_exploded.Event_Name.value_counts(),df_exploded[df_exploded.Event_Name==ET]['Lemma'].value_counts())\n",
    "\n",
    "# df[['SUBJECT_ID', 'LOS_DAYS', 'AGE']].drop_duplicates().mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad65b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[df_exploded.Event_Name==\"Sleep\"].to_excel(\"../exports/temp_sleep_keywords.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_patients = df_exploded[df_exploded.Event_Name==\"Sleep\"].SUBJECT_ID.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bfcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SUBJECT_ID.isin(top10_patients)].to_excel(\"../exports/temp_top10_patients_all_events.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
