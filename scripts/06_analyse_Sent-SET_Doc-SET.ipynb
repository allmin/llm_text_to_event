{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f31e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"P-SET\"\n",
    "llm_type = \"llama3.1:70b\"\n",
    "prompt_version=5\n",
    "prompt_suffix = f\"v{prompt_version}\"\n",
    "fine_analysis = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd235189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrices_for_column_pairs(df, gt_col,vis_columns):\n",
    "\n",
    "    n_pairs = len(vis_columns)\n",
    "\n",
    "    # Determine layout: square-ish grid\n",
    "    n_cols = int(np.ceil(np.sqrt(n_pairs)))\n",
    "    n_rows = int(np.ceil(n_pairs / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    col1 = gt_col\n",
    "    for idx, col2 in enumerate(vis_columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Drop rows with missing values in either column\n",
    "        sub_df = df[[col1, col2]].dropna().copy()\n",
    "        start_length = len(sub_df)\n",
    "        # sub_df = sub_df[sub_df.apply(lambda x: True if (\"_\" not in x[col1] and '_' not in x[col2]) else False, axis=1)]\n",
    "        filter_length = len(sub_df)\n",
    "        # Get confusion matrix\n",
    "        labels = sorted(set(sub_df[col1]) | set(sub_df[col2]))\n",
    "        cm = confusion_matrix(sub_df[col1], sub_df[col2], labels=labels)\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "        ax.set_title(f\"{col1} vs \\n{col2}\\n strt: {start_length}\\nelim.:{filter_length-start_length}\", fontsize=10)\n",
    "        ax.set_xlabel(col2)\n",
    "        ax.set_ylabel(col1)\n",
    "\n",
    "    # Hide any extra axes\n",
    "    for j in range(len(vis_columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cfcadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/asusaiyah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'most_common_unigram': [('orange', 3),\n",
       "  ('apple', 2),\n",
       "  ('ate', 2),\n",
       "  ('babana', 2)],\n",
       " 'most_common_bigram': [(('apple', 'ate'), 2),\n",
       "  (('orange', 'babana'), 2),\n",
       "  (('ate', 'orange'), 1)],\n",
       " 'most_common_trigram': [(('apple', 'ate', 'orange'), 1)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Download tokenizer models if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def most_frequent_ngrams(sentences, N=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        # Remove stopwords and punctuation\n",
    "        filtered_tokens = [t for t in tokens if t not in stop_words and t not in punctuation]\n",
    "\n",
    "        all_unigrams.extend(filtered_tokens)\n",
    "        all_bigrams.extend(ngrams(filtered_tokens, 2))\n",
    "        all_trigrams.extend(ngrams(filtered_tokens, 3))\n",
    "\n",
    "    # Count frequencies\n",
    "    unigram_counts = Counter(all_unigrams)\n",
    "    bigram_counts = Counter(all_bigrams)\n",
    "    trigram_counts = Counter(all_trigrams)\n",
    "\n",
    "    # Get top N most common\n",
    "    most_common_unigram = unigram_counts.most_common(N)\n",
    "    most_common_bigram = bigram_counts.most_common(N)\n",
    "    most_common_trigram = trigram_counts.most_common(N)\n",
    "\n",
    "    return {\n",
    "        'most_common_unigram': most_common_unigram,\n",
    "        'most_common_bigram': most_common_bigram,\n",
    "        'most_common_trigram': most_common_trigram\n",
    "    }\n",
    "\n",
    "\n",
    "most_frequent_ngrams([\"apple ate orange\", \"orange at babana\", \"apple ate\", \"orange at babana to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Sleep************************\n",
      "['../exports/05b_llm_llama3.1:70b_P-SET_v4/Sleep/Sleep_Documents_att_True.pkl', '../exports/05b_llm_llama3.1:70b_P-SET_v4/Sleep/Sleep_Sentences_att_True.pkl']\n",
      "Sleep_Documents_att_True\n",
      "Ao True Sleep_Documents_att_True\n",
      "****************Document**************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_keyword_present\n",
       "0.0    114\n",
       "1.0     82\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc_gt_Sleep  is_keyword_present\n",
      "False         0.0                   110\n",
      "True          1.0                    76\n",
      "False         1.0                     6\n",
      "True          0.0                     4\n",
      "Name: count, dtype: int64\n",
      "Event_Name_Sleep\n",
      "Event_Name_LLM_Events_example_evidence_Doc_Sleep\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['DCTEvent_Name_Sleep'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 229\u001b[0m\n\u001b[1;32m    220\u001b[0m interesting_columns \u001b[38;5;241m=\u001b[39m [id_type, text_type, gt_column, \n\u001b[1;32m    221\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_keyword_present\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDCT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent_Name_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    223\u001b[0m                        col, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText_Quote_in_gt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText_Quote_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m[i\u001b[38;5;241m+\u001b[39mcol_base \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent_Name_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText_Quote_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent_Id_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder_\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \n\u001b[1;32m    228\u001b[0m false_positive_samples \u001b[38;5;241m=\u001b[39m df[(df[col]\u001b[38;5;241m!=\u001b[39mdf[gt_column])\u001b[38;5;241m&\u001b[39m(df[col]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m--> 229\u001b[0m \u001b[43mfalse_positive_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43minteresting_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/FP_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse_positive_keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m,most_frequent_ngrams(flatten_list_of_lists(false_positive_samples[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText_Quote_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()),\u001b[38;5;241m15\u001b[39m))\n\u001b[1;32m    231\u001b[0m false_negative_samples \u001b[38;5;241m=\u001b[39m df[(df[col]\u001b[38;5;241m!=\u001b[39mdf[gt_column])\u001b[38;5;241m&\u001b[39m(df[col]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)]\n",
      "File \u001b[0;32m~/tactics_storage/projects/llm_text_to_event/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/tactics_storage/projects/llm_text_to_event/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/tactics_storage/projects/llm_text_to_event/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['DCTEvent_Name_Sleep'] not in index\""
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from itertools import product\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "\n",
    "disagreement_type = \"correct\"\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
    "def clean_illegal_chars(val):\n",
    "    if isinstance(val, str):\n",
    "        return ILLEGAL_CHARACTERS_RE.sub(\"\", val)\n",
    "    return val\n",
    "\n",
    "def get_time(x):\n",
    "    x = np.array(x)\n",
    "    # Remove NaNs\n",
    "    x = x[~np.isnan(x)]\n",
    "    # If less than 3 values, just return mean\n",
    "    if len(x) < 3:\n",
    "        return np.mean(x)\n",
    "    # Remove outliers using IQR\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    mask = (x >= q1 - 1.5 * iqr) & (x <= q3 + 1.5 * iqr)\n",
    "    return np.mean(x[mask])\n",
    "\n",
    "def fixnames(ls,suffix,remove):\n",
    "    LS=[]\n",
    "    for s in ls:\n",
    "        s = s.replace(\"_keyword_evidence\",\"_Ki\").replace(\"_example_evidence\",\"_Ei\").replace(\"_no_evidence\",\"\").replace(\"_all_evidence\",\"KiEi\").replace(\"Event_Name_\",'').replace(\"_Events\",'_').replace('_'+remove,\"\")\n",
    "        s = s + \"_\" + suffix\n",
    "        if \"sentence\" in s:\n",
    "            s = s.replace(\"sentence\",\"\")\n",
    "            s = s + \"_Sent-SET\"\n",
    "        elif \"document\" in s:\n",
    "            s = s.replace(\"document\",\"\")\n",
    "            s = s + \"_Doc-SET\"\n",
    "        LS.append(s)\n",
    "    return LS\n",
    "\n",
    "def get_col_suffix(keyword_input, example_input):\n",
    "    col_suffix = \"no\"\n",
    "    if keyword_input and example_input:\n",
    "        col_suffix = \"all\"\n",
    "    elif keyword_input and not example_input:\n",
    "        col_suffix = \"keyword\"\n",
    "    elif not keyword_input and example_input:\n",
    "        col_suffix = \"example\"\n",
    "    return col_suffix\n",
    "\n",
    "LLM_dict = {}\n",
    "llm_models_all = []\n",
    "for keyword_input, example_input in [i for i in product([True,False],[True,False])]: \n",
    "    for input_type in [\"Sent\", \"Doc\"]:\n",
    "        col_suffix = get_col_suffix(keyword_input, example_input)\n",
    "        llm_models_all.append(f\"LLM_Events_{col_suffix}_evidence_{input_type}\")\n",
    "df_both = pd.read_pickle(\"../exports/04b_dictionary_features.pkl\")\n",
    "\n",
    "\n",
    "def infer_gt(row, col, ET):\n",
    "    # return 1 if ET in row[col] else 0\n",
    "    if col.replace('Event_Name','Actor') in row:\n",
    "        res = any([1 if ((e==ET and a=='patient') or (e==ET and ET=='Family')) else 0 for e,a in zip(row[col], row[col.replace('Event_Name','Actor')])])\n",
    "    else:\n",
    "        res = 1 if ET in row[col] else 0\n",
    "    return res\n",
    "\n",
    "def ispresent(a,b):\n",
    "    res = []\n",
    "    for i in a:\n",
    "        presence = False\n",
    "        for j in b:\n",
    "            if (i in j) or (j in i):\n",
    "                presence = True\n",
    "                break\n",
    "        res.append(presence)\n",
    "    return res\n",
    "\n",
    "dataset = \"P-SET\"\n",
    "for analysis_type in [\"all\"]: #(M-SET, A-SET, D-SET, F-SET)\n",
    "    for ET in [\"Sleep\"]:\n",
    "        print(f\"********************{ET}************************\")\n",
    "        llm_files = glob(f\"../exports/05b_llm_{llm_type}_{dataset}_{prompt_suffix}{\"_fa\" if fine_analysis else \"\"}/{ET}/*.pkl\")\n",
    "        print(llm_files)\n",
    "        try:\n",
    "            gt_file_sent = glob(f\"../exports/04c_groundtruth/{dataset}/Annotated/{ET}_Sentences.pkl\")[0]\n",
    "            gt_file_doc = glob(f\"../exports/04c_groundtruth/{dataset}/Annotated/{ET}_Documents.pkl\")[0]\n",
    "        except IndexError:\n",
    "            print(f\"No ground truth file found for {ET} in ../exports/04c_groundtruth/{dataset}/Annotated/\")\n",
    "            continue    \n",
    "        \n",
    "        llm_files = [i for i in llm_files if \"True\" in i]\n",
    "        # llm_files = [i for i in llm_files if \"Sent\" in i]\n",
    "        for file in llm_files:\n",
    "            filename = os.path.basename(file).rstrip('.pkl')\n",
    "            print(filename)\n",
    "            text_type = \"Document\" if \"Document\" in filename else \"Sentence\"\n",
    "            _,attribute_requested = filename.split(\"_\")[-2:]\n",
    "            attribute_requested = eval(attribute_requested)\n",
    "            suffix = \"Ao\" if attribute_requested else \"\"           \n",
    "            print(suffix,attribute_requested,filename)\n",
    "            \n",
    "            if text_type == \"Sentence\":\n",
    "                gt_file = gt_file_sent\n",
    "                id_type = \"UID\"\n",
    "                focus = \"Sent\"\n",
    "                gt_column = f\"Sent_gt_{ET}\"\n",
    "            elif text_type == \"Document\":\n",
    "                gt_file = gt_file_doc\n",
    "                id_type = \"ROW_ID\"\n",
    "                focus = \"Doc\"\n",
    "                gt_column = f\"Doc_gt_{ET}\"\n",
    "            print(f\"****************{text_type}**************\")\n",
    "            gt_df = pd.read_pickle(gt_file)\n",
    "            gt_df[id_type] = gt_df[id_type].astype(str)\n",
    "            df_both[id_type] = df_both[id_type].astype(str)\n",
    "            gt_df = gt_df.dropna(subset=gt_column)\n",
    "            if not attribute_requested:\n",
    "                gt_df[gt_column] = gt_df.apply(lambda x: x[gt_column] if x['negation']==False else False,axis=1)\n",
    "            \n",
    "            gt_df['Lemma'] = gt_df['Lemma'].apply(lambda x: tuple(x))\n",
    "            gt_df[\"is_keyword_present\"] = gt_df[\"Event_Name\"].apply(lambda x: 1 if ET in x else 0)\n",
    "            \n",
    "            id_to_gt = {row[id_type]:row[gt_column] for _,row in gt_df.iterrows()}\n",
    "            id_to_negation = {row[id_type]:row['negation'] for _,row in gt_df.iterrows()}\n",
    "            id_to_key_present = {row[id_type]:row[f\"is_keyword_present\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_lemma = {row[id_type]:row[f\"Lemma\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_comment = {row[id_type]:row[f\"comment\"] for _,row in gt_df.iterrows()}\n",
    "            id_to_dict_time = {row[id_type]:row[f\"Time\"] for _,row in df_both.iterrows()}\n",
    "            \n",
    "            output_folder = f\"../exports/06b_analysis/{llm_type}_V{prompt_version}{\"_fa\" if fine_analysis else \"\"}/{ET}\"\n",
    "            os.makedirs(output_folder,exist_ok=True)\n",
    "       \n",
    "            \n",
    "            \n",
    "            df = pd.read_pickle(file)\n",
    "            df[\"num_events_keyword\"] = df[\"Event_Name\"].apply(lambda x: len(x))\n",
    "            df[f\"num_{ET}_keyword\"] = df[\"Event_Name\"].apply(lambda x: len([i for i in x if i == ET]))\n",
    "            df[id_type] = df[id_type].astype(str)\n",
    "            llm_models = [i for i in llm_models_all if i in df]\n",
    "            df[f'{ET}_time'] = df[id_type].apply(lambda x:id_to_dict_time.get(x))\n",
    "            df[\"focus_event\"] = ET\n",
    "            df['negation'] = df[id_type].apply(lambda x:id_to_negation.get(x))\n",
    "            df['comment'] = df[id_type].apply(lambda x:id_to_comment.get(x))\n",
    "            \n",
    "            \n",
    "            df[gt_column] = df[id_type].apply(lambda x:id_to_gt.get(x,None))\n",
    "            df[f\"is_keyword_present\"] = df[id_type].apply(lambda x:id_to_key_present.get(x))\n",
    "            display(df.is_keyword_present.value_counts())\n",
    "            print(df[[gt_column,\"is_keyword_present\"]].value_counts())\n",
    "                        \n",
    "            df[f\"dict_Lemma\"] = df[id_type].apply(lambda x:id_to_lemma.get(x))\n",
    "            df.dropna(subset=gt_column,inplace=True)\n",
    "            df[gt_column] = df[gt_column].astype(int)\n",
    "            \n",
    "            \n",
    "\n",
    "            for col in llm_models:\n",
    "                df[f\"{col}_{ET}_time\"] = df[col].apply(lambda x:x['event_detection_time'])\n",
    "                \n",
    "            if len(df) > 1:\n",
    "                first_row = df.iloc[0]\n",
    "                splittable_columns = [\"Event_Name\"]+[f\"Event_Name_{model}\" for model in llm_models]\n",
    "                disagreement_dfs = []\n",
    "                generated_columns = []\n",
    "\n",
    "                for col in splittable_columns:\n",
    "                    generated_column = f\"{col}_{ET}\"\n",
    "                    df[generated_column] = df.apply(lambda x: infer_gt(x,col,ET), axis=1)\n",
    "                    \n",
    "                    generated_columns.append(generated_column)\n",
    "                \n",
    "                f1s, accs, precs, recs, psup, nsup, times, tp, tn, fp, fn, cia, cif, cip, cir = [],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "                \n",
    "                \n",
    "                for col in generated_columns:\n",
    "                    print(col)\n",
    "                    df_temp = df.copy()\n",
    "                    y_gt = df_temp[gt_column]\n",
    "                    LLM_dict[(attribute_requested,gt_column)] = y_gt\n",
    "                    preds = df_temp[col]\n",
    "                    LLM_dict[(attribute_requested,col)] = preds\n",
    "                    f1s.append(f1_score(y_gt, preds))\n",
    "                    accs.append(accuracy_score(y_gt, preds))\n",
    "                    precs.append(precision_score(y_gt, preds, zero_division=0))\n",
    "                    recs.append(recall_score(y_gt, preds))   \n",
    "                    psup.append(sum(y_gt))\n",
    "                    nsup.append(sum(y_gt==0))\n",
    "                    times.append(get_time(df_temp[f\"{col.lstrip('Event_Name_')}_time\"]))\n",
    "                    tn_i, fp_i, fn_i, tp_i = confusion_matrix(y_gt, preds).ravel()\n",
    "                    prec_ci_low, prec_ci_high = proportion_confint(tp_i, tp_i + fp_i, alpha=0.05, method='normal')\n",
    "                    recall_ci_low, recall_ci_high = proportion_confint(tp_i, tp_i + fn_i, alpha=0.05, method='normal')\n",
    "                    acc_ci_low, acc_ci_high = proportion_confint(tp_i + tn_i, tp_i + fn_i + fp_i + tn_i, alpha=0.05, method='normal')\n",
    "\n",
    "                    tp.append(tp_i)\n",
    "                    tn.append(tn_i)\n",
    "                    fp.append(fp_i)\n",
    "                    fn.append(fn_i)\n",
    "                    cip.append(f\"[{prec_ci_low:0.3f},{prec_ci_high:0.3f}]\")\n",
    "                    cir.append(f\"[{recall_ci_low:0.3f},{recall_ci_high:0.3f}]\")\n",
    "                    cia.append(f\"[{acc_ci_low:0.3f},{acc_ci_high:0.3f}]\")\n",
    "                    \n",
    "                    \n",
    "                    if \"example_evidence\" in col:\n",
    "                        col_base = col.lstrip(\"Event_Name_\").rstrip(f\"_{ET}\")\n",
    "                        df[f\"Text_Quote_{col_base}_{ET}\"] = df.apply(lambda x:[t for t,e in zip(x[f\"Text_Quote_{col_base}\"],x[f\"Event_Name_{col_base}\"]) if e==ET],axis=1)\n",
    "                        df[f\"Attribute_{col_base}_{ET}\"] = df.apply(lambda x:[a for a in (x[f\"Attribute_{col_base}\"]) if list(a.keys())[0]==ET],axis=1)\n",
    "                        df[f\"Event_Id_{col_base}\"] = df[col_base].apply(lambda x:[ f\"{eid}={en}|{1 if tq in x['text'] else 0}:{tq}\" for (eid,en,tq) in zip(x['event_id'],x['event'],x['text_quotes'])])\n",
    "                        df[f\"Text_Quote_in_gt\"] = df.apply(lambda x:ispresent(x[f\"Text_Quote_{col_base}_{ET}\"], x['comment'].split('|')), axis=1)\n",
    "\n",
    "                        interesting_columns = [id_type, text_type, gt_column, \n",
    "                                               \"is_keyword_present\",\"DCT\", col_base,\n",
    "                                               f\"Event_Name_{ET}\", \"comment\", \"negation\", \n",
    "                                               col, \"Text_Quote_in_gt\",\n",
    "                                               f\"Text_Quote_{col_base}_{ET}\",\n",
    "                                               f\"Attribute_{col_base}_{ET}\"]+[i+col_base for i in [\"Event_Name_\",\"Attribute_\",\"Text_Quote_\", \"Event_Id_\",\"Order_\"]] \n",
    "                        \n",
    "                        \n",
    "                        false_positive_samples = df[(df[col]!=df[gt_column])&(df[col]==1)]\n",
    "                        false_positive_samples[interesting_columns].to_excel(f\"{output_folder}/FP_{col}.xlsx\")\n",
    "                        print(\"false_positive_keywords:\",most_frequent_ngrams(flatten_list_of_lists(false_positive_samples[f\"Text_Quote_{col_base}_{ET}\"].to_list()),15))\n",
    "                        false_negative_samples = df[(df[col]!=df[gt_column])&(df[col]==0)]\n",
    "                        false_negative_samples[interesting_columns].to_excel(f\"{output_folder}/FN_{col}.xlsx\")\n",
    "                        # print(\"false_negative_keywords:\",most_frequent_ngrams(flatten_list_of_lists(false_positive_samples[f\"comment\"].to_list()),10))\n",
    "                        true_positive_samples = df[(df[col]==df[gt_column])&(df[gt_column]==1)]\n",
    "                        true_positive_samples[interesting_columns].to_excel(f\"{output_folder}/TP_{col}.xlsx\")\n",
    "                        true_negative_samples = df[(df[col]==df[gt_column])&(df[gt_column]==0)]\n",
    "                        true_negative_samples[interesting_columns].to_excel(f\"{output_folder}/TN_{col}.xlsx\")\n",
    "                        print(\"TP\",len(true_positive_samples),\"TN\",len(true_negative_samples),\"FP\",len(false_positive_samples),\"FN\",len(false_negative_samples),\"%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "           \n",
    "                results_df = {\"col_name\":generated_columns, \"technique\":fixnames(generated_columns,suffix,ET), \"pos_sup\": psup, \"neg_sup\": nsup, \"accuracy\":accs,\"accuracy-CI\":cia,\"f1_score\":f1s, \"precision\":precs, \"precision-CI\":cip,\"recall\":recs, \"recall-CI\":cir, \"TP\":tp, 'TN':tn, 'FP':fp, 'FN':fn, \"time\":times} \n",
    "                results = pd.DataFrame(results_df)   \n",
    "                plot_confusion_matrices_for_column_pairs(df, gt_column,vis_columns=generated_columns)\n",
    "                op_path = f\"{output_folder}/{analysis_type}_{disagreement_type}_{filename}.xlsx\"\n",
    "                df = df.applymap(clean_illegal_chars)\n",
    "                df.to_excel(op_path,index=False)\n",
    "                print(f\"file written to {op_path}\")\n",
    "                display(results)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_confint(451, 451 + 87, alpha=0.05, method='normal'),proportion_confint(439, 439 + 47, alpha=0.05, method='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8770868",
   "metadata": {},
   "outputs": [],
   "source": [
    "439 +47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splittable_columns[1].replace('Event_Name','Actor') in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a036f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_samples[text_type].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06630c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f\"Text_Quote_in_gt\"] = df.apply(lambda x:[((i in j) or (j in i)) for (i,j) in product(x[f\"Text_Quote_{col_base}_{ET}\"], x['comment'])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c910cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_samples[interesting_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.LLM_Events_example_evidence_Sent.iloc[2]['event_name_prompt'])\n",
    "print(df.LLM_Events_example_evidence_Sent.iloc[2]['raw_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET='Sleep'\n",
    "y_gt = df.negation\n",
    "preds=df.apply(lambda x: any([1 if (e==ET and n==True) else 0 for (e,n) in zip(x['Event_Name_LLM_Events_example_evidence_Sent'],x['Negation_LLM_Events_example_evidence_Sent'])]),axis=1)\n",
    "precision_score(y_gt, preds, zero_division=0),recall_score(y_gt, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87565a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LLM_Events_example_evidence_Sent'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239acf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = [2,3,4,2]\n",
    "jj.index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecffd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[col]==df[gt_column])&(df[col]==1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{col}_{ET}\", df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[f\"{col.lstrip('Event_Name_')}_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "afile=\"../exports/06_analysis/Sleep/all_correct_Sleep_Sentences_att_True.xlsx\"\n",
    "amodel = \"Event_Name_LLM_Events_all_evidence_Sent_Sleep\"\n",
    "agt = \"Sent_gt_Sleep\"\n",
    "\n",
    "adf = pd.read_excel(afile)\n",
    "adf[adf[amodel]!=adf[agt]][['Sentence',agt,amodel]].to_excel(f\"../exports/temp_false_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_gt = {row[id_type]:row[gt_column] for _,row in gt_df.iterrows()}\n",
    "set(id_to_gt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ea880",
   "metadata": {},
   "outputs": [],
   "source": [
    "row['Sent_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{output_folder}/{analysis_type}_{disagreement_type}_{filename}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df_temp[(df_temp[gt_column]==False) & (df_temp[col]==True) & (df_temp[\"Event_Name_Sleep\"]==False)][[id_type,text_type, gt_column, col]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(file)\n",
    "llm_models = [i for i in llm_models_all if i in df]\n",
    "file,llm_models, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b05b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_type =\"Document\" \n",
    "file = f'../exports/05b_llm_llama3.1:70b_P-SET/Sleep/Sleep_{text_type}s_att_True.pkl'\n",
    "llm_df = pd.read_pickle(file)\n",
    "for llm_model in llm_models:\n",
    "    llm_df['tqvalid']=llm_df.apply(lambda x:[i in x[text_type] for i in x[f\"Text_Quotes_{llm_model}\"]], axis=1)\n",
    "    print(llm_model, llm_df['tqvalid'].explode().value_counts())\n",
    "# Text_Quotes_LLM_Events_all_evidence_Sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9184bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"DOCUMENT\",\"Event_Name_LLM_Events_keyword_evidence_document_Ao\",\"Sentence\" ,\"GT_Sleep\"]].to_excel(\"../exports/temp_result.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38140d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'dictionary_Sleep_time' in df.columns, llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3aff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac5c04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d947a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.Keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86164cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary = pd.read_pickle(f\"../exports/03b_selected_reports_with_event_log_only_dictionary_v2/combined.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary['Events'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "ET = \"Sleep\"\n",
    "df = pd.read_pickle(glob(f\"../exports/04b_groundtruth/P-SET/Generated/{ET}*.pkl\")[0])\n",
    "print(df.UID.nunique(), df.Sentence.nunique(), df.ROW_ID.nunique(), df.SUBJECT_ID.nunique(), df.Event_Name.value_counts())\n",
    "\n",
    "df_exploded = df.explode(['Keyword','Lemma','Event_Name','Keyword_Position'])\n",
    "df_exploded['KUID'] = df_exploded['UID'] + \"_\" + df_exploded['Keyword_Position'].astype(str)\n",
    "print(df_exploded.Event_Name.value_counts(),df_exploded[df_exploded.Event_Name==ET]['Lemma'].value_counts())\n",
    "\n",
    "# df[['SUBJECT_ID', 'LOS_DAYS', 'AGE']].drop_duplicates().mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad65b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[df_exploded.Event_Name==\"Sleep\"].to_excel(\"../exports/temp_sleep_keywords.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_patients = df_exploded[df_exploded.Event_Name==\"Sleep\"].SUBJECT_ID.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bfcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SUBJECT_ID.isin(top10_patients)].to_excel(\"../exports/temp_top10_patients_all_events.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
