{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385ae15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file len:  14483\n",
      "After exploding file len:  322079\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from config import event_types\n",
    "random.seed(42)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# jupyter nbconvert --to script 04b_combine_results_find_disagreement.ipynb\n",
    "# sudo kill -9 $(nvidia-smi | awk 'NR>8 {print $5}' | grep -E '^[0-9]+$')\n",
    "version = 2\n",
    "from config import event_types\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "def prepare_df(df, type=\"biolord\"):\n",
    "    global event_types\n",
    "    df = df.copy()\n",
    "    print(\"Loaded file len: \",len(df))\n",
    "    df['Sent_ID'] = df['Events'].apply(lambda x: [f\"{i:04d}\" for i in range(len(x))])\n",
    "    df = df.explode([\"Sent_ID\",\"Events\"])\n",
    "    print(\"After exploding file len: \",len(df))\n",
    "    df['UID'] = df['SUBJECT_ID'].astype(str) + \"_\" + df['ROW_ID'].astype(str) + \"_\" + df['Sent_ID'].astype(str)\n",
    "    df = df.dropna(subset=\"Events\")\n",
    "    df['Event_Name'] = df['Events'].apply(lambda x: x['event'])\n",
    "    df['Sentence'] = df['Events'].apply(lambda x: x['text'])\n",
    "    df['Time'] = df['Events'].apply(lambda x: x['event_detection_time'])\n",
    "    \n",
    "    if type == \"dictionary\":\n",
    "        df['Keyword'] = df['Events'].apply(lambda x: x['keyword'])\n",
    "        df['Lemma'] = df['Events'].apply(lambda x: x['lemma'])\n",
    "        df['Keyword_Position'] = df['Events'].apply(lambda x: x['keyword_position'])\n",
    "        \n",
    "    if type == \"biolord\":\n",
    "        df['Similarity'] = df['Events'].apply(lambda x: x['similarity'])\n",
    "        df['Similarity'] = df['Similarity'].apply(lambda x: {k:v for (k,v) in x.items() if k!=\"Alert And Oriented\"})\n",
    "        for ET in event_types:\n",
    "            df[f\"{ET}_similarity\"] = df['Similarity'].apply(lambda x:x[ET])\n",
    "            \n",
    "    return df\n",
    "df_dictionary = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_dictionary_v{version}/combined.pkl\"),type=\"dictionary\")\n",
    "# df_biolord = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_biolord_v{version}/combined.pkl\"), type = \"biolord\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ccd116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
       "       'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'DOCUMENT',\n",
       "       'AGE', 'LOS_DAYS', 'IS_ALIVE', 'DOCUMENT_LOWER', 'NUM_NOTES',\n",
       "       'MAX_NOTES_PER_DAY', 'MAX_NOTES_PER_CHARTTIME', 'all_dates_present',\n",
       "       'Sentences', 'Sentences_Raw', 'Sentences_Cleaned', 'Events', 'Sent_ID',\n",
       "       'UID', 'Event_Name', 'Sentence', 'Time', 'Keyword', 'Lemma',\n",
       "       'Keyword_Position'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dictionary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625e4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary_exploded = df_dictionary.explode(['Keyword','Lemma','Event_Name','Keyword_Position'])\n",
    "df_dictionary_exploded['KUID'] = df_dictionary_exploded['UID'] + \"_\" + df_dictionary_exploded['Keyword_Position'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac88677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>...</th>\n",
       "      <th>Sentences_Cleaned</th>\n",
       "      <th>Events</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>UID</th>\n",
       "      <th>Event_Name</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Time</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Keyword_Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333811</td>\n",
       "      <td>26851</td>\n",
       "      <td>169435</td>\n",
       "      <td>2139-07-13</td>\n",
       "      <td>2139-07-13 10:19:00</td>\n",
       "      <td>2139-07-13 10:20:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>17745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[subarachnoid hemorrhage (sah), assessment:  s...</td>\n",
       "      <td>{'text': 'subarachnoid hemorrhage (sah)', 'eve...</td>\n",
       "      <td>0000</td>\n",
       "      <td>26851_333811_0000</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>subarachnoid hemorrhage (sah)</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333811</td>\n",
       "      <td>26851</td>\n",
       "      <td>169435</td>\n",
       "      <td>2139-07-13</td>\n",
       "      <td>2139-07-13 10:19:00</td>\n",
       "      <td>2139-07-13 10:20:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>17745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[subarachnoid hemorrhage (sah), assessment:  s...</td>\n",
       "      <td>{'text': 'assessment:  stable neurologically i...</td>\n",
       "      <td>0001</td>\n",
       "      <td>26851_333811_0001</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>assessment:  stable neurologically intact</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333811</td>\n",
       "      <td>26851</td>\n",
       "      <td>169435</td>\n",
       "      <td>2139-07-13</td>\n",
       "      <td>2139-07-13 10:19:00</td>\n",
       "      <td>2139-07-13 10:20:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>17745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[subarachnoid hemorrhage (sah), assessment:  s...</td>\n",
       "      <td>{'text': 'action:  zofran, raglan, mso4 given ...</td>\n",
       "      <td>0002</td>\n",
       "      <td>26851_333811_0002</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>action:  zofran, raglan, mso4 given for modera...</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333811</td>\n",
       "      <td>26851</td>\n",
       "      <td>169435</td>\n",
       "      <td>2139-07-13</td>\n",
       "      <td>2139-07-13 10:19:00</td>\n",
       "      <td>2139-07-13 10:20:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>17745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[subarachnoid hemorrhage (sah), assessment:  s...</td>\n",
       "      <td>{'text': 'response:  improved per patient', 'e...</td>\n",
       "      <td>0003</td>\n",
       "      <td>26851_333811_0003</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>response:  improved per patient</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333811</td>\n",
       "      <td>26851</td>\n",
       "      <td>169435</td>\n",
       "      <td>2139-07-13</td>\n",
       "      <td>2139-07-13 10:19:00</td>\n",
       "      <td>2139-07-13 10:20:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>17745.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[subarachnoid hemorrhage (sah), assessment:  s...</td>\n",
       "      <td>{'text': 'plan:  initiate pos', 'event': ['Unk...</td>\n",
       "      <td>0004</td>\n",
       "      <td>26851_333811_0004</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>plan:  initiate pos</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID  CHARTDATE            CHARTTIME  \\\n",
       "0  333811       26851   169435 2139-07-13  2139-07-13 10:19:00   \n",
       "0  333811       26851   169435 2139-07-13  2139-07-13 10:19:00   \n",
       "0  333811       26851   169435 2139-07-13  2139-07-13 10:19:00   \n",
       "0  333811       26851   169435 2139-07-13  2139-07-13 10:19:00   \n",
       "0  333811       26851   169435 2139-07-13  2139-07-13 10:19:00   \n",
       "\n",
       "             STORETIME CATEGORY            DESCRIPTION     CGID  ISERROR  ...  \\\n",
       "0  2139-07-13 10:20:01  Nursing  Nursing Progress Note  17745.0      NaN  ...   \n",
       "0  2139-07-13 10:20:01  Nursing  Nursing Progress Note  17745.0      NaN  ...   \n",
       "0  2139-07-13 10:20:01  Nursing  Nursing Progress Note  17745.0      NaN  ...   \n",
       "0  2139-07-13 10:20:01  Nursing  Nursing Progress Note  17745.0      NaN  ...   \n",
       "0  2139-07-13 10:20:01  Nursing  Nursing Progress Note  17745.0      NaN  ...   \n",
       "\n",
       "                                   Sentences_Cleaned  \\\n",
       "0  [subarachnoid hemorrhage (sah), assessment:  s...   \n",
       "0  [subarachnoid hemorrhage (sah), assessment:  s...   \n",
       "0  [subarachnoid hemorrhage (sah), assessment:  s...   \n",
       "0  [subarachnoid hemorrhage (sah), assessment:  s...   \n",
       "0  [subarachnoid hemorrhage (sah), assessment:  s...   \n",
       "\n",
       "                                              Events  Sent_ID  \\\n",
       "0  {'text': 'subarachnoid hemorrhage (sah)', 'eve...     0000   \n",
       "0  {'text': 'assessment:  stable neurologically i...     0001   \n",
       "0  {'text': 'action:  zofran, raglan, mso4 given ...     0002   \n",
       "0  {'text': 'response:  improved per patient', 'e...     0003   \n",
       "0  {'text': 'plan:  initiate pos', 'event': ['Unk...     0004   \n",
       "\n",
       "                 UID Event_Name  \\\n",
       "0  26851_333811_0000  [Unknown]   \n",
       "0  26851_333811_0001  [Unknown]   \n",
       "0  26851_333811_0002  [Unknown]   \n",
       "0  26851_333811_0003  [Unknown]   \n",
       "0  26851_333811_0004  [Unknown]   \n",
       "\n",
       "                                            Sentence      Time  Keyword  \\\n",
       "0                      subarachnoid hemorrhage (sah)  0.056522       []   \n",
       "0          assessment:  stable neurologically intact  0.002603       []   \n",
       "0  action:  zofran, raglan, mso4 given for modera...  0.002590       []   \n",
       "0                    response:  improved per patient  0.002463       []   \n",
       "0                                plan:  initiate pos  0.002435       []   \n",
       "\n",
       "   Lemma Keyword_Position  \n",
       "0     []               []  \n",
       "0     []               []  \n",
       "0     []               []  \n",
       "0     []               []  \n",
       "0     []               []  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fd6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Pain*** Pain counts:422\n",
      " Unique Sentences: 5620\n",
      " Unique Reports: 225\n",
      " Unique Patients: 10\n",
      "\n",
      "***Sleep*** Sleep counts:164\n",
      " Unique Sentences: 4783\n",
      " Unique Reports: 210\n",
      " Unique Patients: 10\n",
      "\n",
      "***Excretion*** Excretion counts:229\n",
      " Unique Sentences: 7382\n",
      " Unique Reports: 317\n",
      " Unique Patients: 10\n",
      "\n",
      "***Eating*** Eating counts:152\n",
      " Unique Sentences: 7267\n",
      " Unique Reports: 329\n",
      " Unique Patients: 10\n",
      "\n",
      "***Family*** Family counts:536\n",
      " Unique Sentences: 7910\n",
      " Unique Reports: 328\n",
      " Unique Patients: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "from config import event_types\n",
    "import os\n",
    "os.makedirs(\"../exports/groundtruth/P-SET/Generated\", exist_ok=True)\n",
    "for ET in event_types:\n",
    "    top_ET = df_dictionary_exploded[df_dictionary_exploded.Event_Name==ET][[\"SUBJECT_ID\",\"Event_Name\"]].value_counts().reset_index().iloc[:N]\n",
    "    selected_patient_sentences = df_dictionary[df_dictionary.SUBJECT_ID.isin(top_ET['SUBJECT_ID'].tolist())]\n",
    "    total_ET = top_ET[top_ET.Event_Name==ET]['count'].sum()\n",
    "    num_sentences = len(selected_patient_sentences.UID.unique())\n",
    "    num_reports = len(selected_patient_sentences.ROW_ID.unique())\n",
    "    num_patients = len(selected_patient_sentences.SUBJECT_ID.unique())\n",
    "    print(f\"***{ET}***\",\n",
    "          f\"{ET} counts:{total_ET}\\n\",\n",
    "        # f\"Event counts:\\n{selected_patient_sentences.Event_Name.value_counts()}\\n\"\n",
    "        f\"Unique Sentences: {num_sentences}\\n\",\n",
    "        f\"Unique Reports: {num_reports}\\n\",\n",
    "        f\"Unique Patients: {num_patients}\\n\"\n",
    "    )\n",
    "    selected_patient_sentences.to_pickle(f\"../exports/groundtruth/P-SET/Generated/{ET}_{num_patients}_{num_reports}_{num_sentences}_{total_ET}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d6d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c4cbbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1135_1277059_0000', '1135_1277059_0001', '1135_1277059_0002', ...,\n",
       "       '32291_1671540_0018', '32291_1671540_0019', '32291_1671540_0020'],\n",
       "      shape=(4783,), dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_patient_sentences.UID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc26671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72c9485b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
