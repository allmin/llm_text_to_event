{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385ae15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file len:  12524\n",
      "After exploding file len:  280066\n",
      "Loaded file len:  12524\n",
      "After exploding file len:  280066\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from config import event_types\n",
    "random.seed(42)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# jupyter nbconvert --to script 04b_combine_results_find_disagreement.ipynb\n",
    "# sudo kill -9 $(nvidia-smi | awk 'NR>8 {print $5}' | grep -E '^[0-9]+$')\n",
    "version = 2\n",
    "from config import event_types\n",
    "\n",
    "def prepare_df(df, type=\"biolord\"):\n",
    "    global event_types\n",
    "    df = df.copy()\n",
    "    print(\"Loaded file len: \",len(df))\n",
    "    df.columns = ['Document' if i=='DOCUMENT' else i for i in df.columns]\n",
    "    df['Sent_ID'] = df['Events'].apply(lambda x: [f\"{i:04d}\" for i in range(len(x))])\n",
    "    df = df.explode([\"Sent_ID\",\"Events\"])\n",
    "    print(\"After exploding file len: \",len(df))\n",
    "    df['UID'] = df['SUBJECT_ID'].astype(str) + \"_\" + df['ROW_ID'].astype(str) + \"_\" + df['Sent_ID'].astype(str)\n",
    "    df = df.dropna(subset=\"Events\")\n",
    "    df['Event_Name'] = df['Events'].apply(lambda x: x['event'])\n",
    "    df['Sentence'] = df['Events'].apply(lambda x: x['text'])\n",
    "    df['Time'] = df['Events'].apply(lambda x: x['event_detection_time'])\n",
    "    \n",
    "    if type == \"dictionary\":\n",
    "        df['Keyword'] = df['Events'].apply(lambda x: x['keyword'])\n",
    "        df['Lemma'] = df['Events'].apply(lambda x: x['lemma'])\n",
    "        df['Keyword_Position'] = df['Events'].apply(lambda x: x['keyword_position'])\n",
    "        \n",
    "    if type == \"biolord\":\n",
    "        df['Similarity'] = df['Events'].apply(lambda x: x['similarity'])\n",
    "        df['Similarity'] = df['Similarity'].apply(lambda x: {k:v for (k,v) in x.items() if k!=\"Alert And Oriented\"})\n",
    "        for ET in event_types:\n",
    "            df[f\"{ET}_similarity\"] = df['Similarity'].apply(lambda x:x[ET])\n",
    "            \n",
    "    return df\n",
    "df_dictionary = prepare_df(pd.read_pickle(f\"../exports/03b_selected_reports_with_event_log_only_dictionary_v{version}_orig/combined.pkl\"),type=\"dictionary\")\n",
    "df_dictionary_wc = prepare_df(pd.read_pickle(f\"../exports/03b_selected_reports_with_event_log_only_dictionary_v{version}_withcontext/combined.pkl\"),type=\"dictionary\")\n",
    "\n",
    "# df_biolord = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_biolord_v{version}/combined.pkl\"), type = \"biolord\")\n",
    "df_dictionary.to_pickle(\"../exports/04b_dictionary_features.pkl\")\n",
    "df_dictionary_wc.to_pickle(\"../exports/04b_dictionary_features_withcontext.pkl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a011af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary = pd.read_pickle(\"../exports/04b_dictionary_features.pkl\")\n",
    "df_dictionary_wc = pd.read_pickle(\"../exports/04b_dictionary_features_withcontext.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625e4b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n",
      "Sentences from top N1:  150 Sentences from rest, with ET:  150 Sentences from rest, with ET:  150\n",
      "Sentences without et but with et context:  300 Sentences without et:  150\n",
      "For event type Sleep, N1=10, Sentences_with_ET=450, Sentences_without_ET=450, Sentences=900,Documents_with_ET=83, Documents_without_ET=119, Documents=202\n",
      "900 previously annotated: 450\n",
      "202 202\n",
      "Sent_gt_Sleep\n",
      "None     450\n",
      "True     251\n",
      "False    191\n",
      "NaN        8\n",
      "Name: count, dtype: int64 Doc_gt_Sleep\n",
      "False    113\n",
      "True      77\n",
      "NaN       12\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3687631/1721388753.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Documents_with_ET['is_keyword_present'] = True\n",
      "/tmp/ipykernel_3687631/1721388753.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Documents_without_ET['is_keyword_present'] = False\n"
     ]
    }
   ],
   "source": [
    "N1 = 10\n",
    "def combine_lists(x):\n",
    "    combined = []\n",
    "    for item in x:\n",
    "        if item:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                combined.extend(item)\n",
    "            else:\n",
    "                combined.append(item)\n",
    "    combined = [i for i in combined if i]\n",
    "    res = set(combined)\n",
    "    return list(res)\n",
    "from config import event_types\n",
    "import os\n",
    "os.makedirs(\"../exports/04c_groundtruth/P-SET/Generated\", exist_ok=True)\n",
    "event_types = ['Sleep']\n",
    "for ET in event_types:\n",
    "    df_dictionary[f'Event_Name_{ET}'] = df_dictionary['Event_Name'].apply(lambda x: ET in x)\n",
    "    df_dictionary_wc[f'Event_Name_{ET}'] = df_dictionary_wc['Event_Name'].apply(lambda x: f\"{ET}\" in x)\n",
    "    df_dictionary_wc[f'Event_Name_{ET}context'] = df_dictionary_wc['Event_Name'].apply(lambda x: f\"{ET}context\" in x)\n",
    "    row_id_to_doc_ET = df_dictionary.groupby('ROW_ID')[f'Event_Name_{ET}'].max().to_dict()\n",
    "    df_dictionary[f'Event_Name_{ET}_doc'] = df_dictionary['ROW_ID'].apply(lambda x: row_id_to_doc_ET[x])\n",
    "    people_ranked_by_ET = df_dictionary.groupby('SUBJECT_ID')[f'Event_Name_{ET}'].sum().sort_values(ascending=False).index.tolist()\n",
    "    df_dictionary_topN1 = df_dictionary[df_dictionary['SUBJECT_ID'].isin(people_ranked_by_ET[:N1])]\n",
    "    sentences_w_et_topn1 = df_dictionary_topN1[df_dictionary_topN1[f'Event_Name_{ET}']]\n",
    "    df_dictionary_rest = df_dictionary[~df_dictionary['SUBJECT_ID'].isin(people_ranked_by_ET[:N1])]\n",
    "    sentences_w_et_rest = df_dictionary_rest[df_dictionary_rest[f'Event_Name_{ET}']].sample(len(sentences_w_et_topn1), random_state=42)\n",
    "    print(len(sentences_w_et_topn1), len(sentences_w_et_rest))\n",
    "    Sentences_with_ET = pd.concat([sentences_w_et_topn1, sentences_w_et_rest]).reset_index(drop=True)\n",
    "    Sentences_with_ET['is_keyword_present'] = True\n",
    "    df_dictionary_rest2 = df_dictionary[~df_dictionary.UID.isin(Sentences_with_ET.UID)]\n",
    "    sentences_w_et_rest2 = df_dictionary_rest2[df_dictionary_rest2[f'Event_Name_{ET}']].sample(len(sentences_w_et_topn1), random_state=42)\n",
    "    Sentences_with_ET = pd.concat([Sentences_with_ET, sentences_w_et_rest2]).reset_index(drop=True)\n",
    "    Sentences_with_ET['is_keyword_present'] = True\n",
    "    print(\"Sentences from top N1: \",len(sentences_w_et_topn1), \n",
    "          \"Sentences from rest, with ET: \",len(sentences_w_et_rest), \n",
    "          \"Sentences from rest, with ET: \",len(sentences_w_et_rest2))\n",
    "\n",
    "    sentences_wo_et_w_context = df_dictionary_wc[(df_dictionary_wc[f'Event_Name_{ET}']==False)&(df_dictionary_wc[f'Event_Name_{ET}context']==True)]\n",
    "    sentences_wo_et_w_context_sample = sentences_wo_et_w_context.sample(2*len(sentences_w_et_topn1), random_state=42)\n",
    "    sentences_wo_et_topn1 = (df_dictionary_topN1[df_dictionary_topN1[f'Event_Name_{ET}']==False]\n",
    "                            .sample(2*len(sentences_w_et_topn1), random_state=42))\n",
    "    sentences_wo_et_topn1_sample = sentences_wo_et_topn1[~sentences_wo_et_topn1.UID.isin(sentences_wo_et_w_context.UID)].iloc[:len(sentences_w_et_topn1)]\n",
    "    Sentences_without_ET = pd.concat([sentences_wo_et_w_context_sample, sentences_wo_et_topn1_sample]).reset_index(drop=True)\n",
    "    Sentences_without_ET['is_keyword_present'] = False\n",
    "    print(\"Sentences without et but with et context: \",len(sentences_wo_et_w_context_sample), \n",
    "          \"Sentences without et: \",len(sentences_wo_et_topn1_sample))\n",
    "    Sentences = pd.concat([Sentences_with_ET, Sentences_without_ET]).reset_index(drop=True)\n",
    "    \n",
    "    Sentences[f'Sent_gt_{ET}'] = None\n",
    "    Sentences['negation'] = False\n",
    "    Sentences['good_example'] = False\n",
    "    Sentences[\"comment\"] = \"\"\n",
    "    \n",
    "    Documents_with_ET = df_dictionary_topN1[df_dictionary_topN1[f'Event_Name_{ET}_doc']]\n",
    "    Documents_with_ET['is_keyword_present'] = True\n",
    "    Documents_without_ET =  df_dictionary_topN1[~df_dictionary_topN1[f'Event_Name_{ET}_doc']]\n",
    "    Documents_without_ET['is_keyword_present'] = False\n",
    "    Documents = pd.concat([Documents_with_ET, Documents_without_ET]).reset_index(drop=True)\n",
    "    Documents['Document'] = Documents['Document'].apply(lambda x: x.lower())\n",
    "    Documents = Documents.groupby(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
    "       'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'AGE', 'LOS_DAYS', 'IS_ALIVE', 'DURATION_NOTE'])[[\"Event_Name\",\"Keyword\",\"Document\",\"Lemma\"]].agg(lambda x:combine_lists(x)).reset_index()\n",
    "    Documents['Document'] = [i[0] for i in Documents['Document']]\n",
    "    print(f\"For event type {ET}, N1={N1}, Sentences_with_ET={len(Sentences_with_ET)}, Sentences_without_ET={len(Sentences_without_ET)}, Sentences={len(Sentences)},Documents_with_ET={Documents_with_ET.ROW_ID.nunique()}, Documents_without_ET={Documents_without_ET.ROW_ID.nunique()}, Documents={Documents.ROW_ID.nunique()}\")\n",
    "    \n",
    "    if os.path.exists(f\"../exports/04b_groundtruth/P-SET/Annotated/{ET}_Sentences.pkl\"):\n",
    "        old_annotations_sent = pd.read_pickle(f\"../exports/04b_groundtruth/P-SET/Annotated/{ET}_Sentences.pkl\")\n",
    "        uid_to_gt = old_annotations_sent.groupby(['UID'])[f'Sent_gt_{ET}'].max().to_dict()\n",
    "        uid_to_negation = old_annotations_sent.groupby(['UID'])[f'negation'].max().to_dict()\n",
    "        uid_to_good_example = old_annotations_sent.groupby(['UID'])[f'good_example'].max().to_dict()\n",
    "        uid_to_comment = old_annotations_sent.groupby(['UID'])[f'comment'].max().to_dict()\n",
    "        Sentences[f'Sent_gt_{ET}'] = Sentences.UID.map(lambda x: uid_to_gt.get(x, None))\n",
    "        Sentences['negation'] = Sentences.UID.map(lambda x: uid_to_negation.get(x, False))\n",
    "        Sentences['good_example'] = Sentences.UID.map(lambda x: uid_to_good_example.get(x, False))\n",
    "        Sentences['comment'] = Sentences.UID.map(lambda x: uid_to_comment.get(x, \"\"))\n",
    "        print(len(Sentences), \"previously annotated:\", len(Sentences[Sentences.UID.isin(old_annotations_sent.UID)]))\n",
    "        \n",
    "    if os.path.exists(f\"../exports/04b_groundtruth/P-SET/Annotated/{ET}_Documents.pkl\"):\n",
    "        old_annotations_doc = pd.read_pickle(f\"../exports/04b_groundtruth/P-SET/Annotated/{ET}_Documents.pkl\")\n",
    "        row_id_to_gt = old_annotations_doc.groupby(['ROW_ID'])[f'Doc_gt_{ET}'].max().to_dict()\n",
    "        row_id_to_negation = old_annotations_doc.groupby(['ROW_ID'])[f'negation'].max().to_dict()\n",
    "        row_id_to_good_example = old_annotations_doc.groupby(['ROW_ID'])[f'good_example'].max().to_dict()\n",
    "        row_id_to_comment = old_annotations_doc.groupby(['ROW_ID'])[f'comment'].max().to_dict()\n",
    "        Documents[f'Doc_gt_{ET}'] = Documents.ROW_ID.map(lambda x: row_id_to_gt.get(x,None))\n",
    "        Documents['negation'] = Documents.ROW_ID.map(lambda x: row_id_to_negation.get(x,False))\n",
    "        Documents['good_example'] = Documents.ROW_ID.map(lambda x: row_id_to_good_example.get(x,False))\n",
    "        Documents['comment'] = Documents.ROW_ID.map(lambda x: row_id_to_comment.get(x,\"\"))\n",
    "        print(len(Documents), len(Documents[Documents.ROW_ID.isin(old_annotations_doc.ROW_ID)]))\n",
    "        \n",
    "    if ET == \"Sleep\":\n",
    "        print(Sentences.Sent_gt_Sleep.value_counts(dropna=False), Documents.Doc_gt_Sleep.value_counts(dropna=False))\n",
    "    Sentences.to_pickle(f\"../exports/04c_groundtruth/P-SET/Generated/{ET}_Sentences.pkl\")\n",
    "    Documents.to_pickle(f\"../exports/04c_groundtruth/P-SET/Generated/{ET}_Documents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47feeabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [UID, Sentence]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_wo_et_topn1_sample[sentences_wo_et_topn1_sample.UID.isin(sentences_wo_et_w_context.UID)][['UID','Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87018753",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_annotations_sent.head(1), Sentences.head(1)\n",
    "# sent_to_negation.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET='Sleep'\n",
    "os.path.exists(f\"../exports/04_groundtruth/P-SET/Annotated/{ET}_Documents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = \"Sleep\"\n",
    "old_annotations = pd.read_pickle(f\"../exports/04_groundtruth/P-SET/Annotated/{ET}_Documents.pkl\")\n",
    "new_annotations = pd.read_pickle(f\"../exports/04b_groundtruth/P-SET/Generated/{ET}_Documents.pkl\")\n",
    "\n",
    "sent_to_gt = old_annotations.groupby('Document')[f'Doc_gt_{ET}'].max().to_dict()\n",
    "len([i for i in new_annotations.Document.tolist() if i in old_annotations.Document.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations.Sentence.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fafbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dictionary_topN2[~df_dictionary_topN2[f'Event_Name_{ET}']]), len(Sentences_with_ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac88677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents.Document.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 10\n",
    "# from config import event_types\n",
    "# import os\n",
    "# os.makedirs(\"../exports/04_groundtruth/P-SET/Generated\", exist_ok=True)\n",
    "# for ET in event_types:\n",
    "#     top_ET = df_dictionary_exploded[df_dictionary_exploded.Event_Name==ET][[\"SUBJECT_ID\",\"Event_Name\"]].value_counts().reset_index().iloc[:N]\n",
    "#     selected_patient_sentences = df_dictionary[df_dictionary.SUBJECT_ID.isin(top_ET['SUBJECT_ID'].tolist())]\n",
    "#     total_ET = top_ET[top_ET.Event_Name==ET]['count'].sum()\n",
    "#     num_sentences = len(selected_patient_sentences.UID.unique())\n",
    "#     num_reports = len(selected_patient_sentences.ROW_ID.unique())\n",
    "#     num_patients = len(selected_patient_sentences.SUBJECT_ID.unique())\n",
    "#     print(f\"***{ET}***\",\n",
    "#           f\"{ET} counts:{total_ET}\\n\",\n",
    "#         # f\"Event counts:\\n{selected_patient_sentences.Event_Name.value_counts()}\\n\"\n",
    "#         f\"Unique Sentences: {num_sentences}\\n\",\n",
    "#         f\"Unique Reports: {num_reports}\\n\",\n",
    "#         f\"Unique Patients: {num_patients}\\n\"\n",
    "#     )\n",
    "#     selected_patient_sentences.to_pickle(f\"../exports/04_groundtruth/P-SET/Generated/04_{ET}_{num_patients}_{num_reports}_{num_sentences}_{total_ET}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cbc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"../exports/04_dictionary_features.pkl\")\n",
    "ET = 'Sleep'\n",
    "df_sentence_with_sleep = (df[df.Event_Name.apply(lambda x: ET in x)])\n",
    "df_sentence_without_sleep = (df[df.Event_Name.apply(lambda x: ET not in x)])\n",
    "top_10_patients = df_sentence_with_sleep.SUBJECT_ID.value_counts().head(10).index.tolist()\n",
    "all_patients = pd.read_pickle(\"../exports/02b_filtered_patient_reports_7_14_days.pkl\")['SUBJECT_ID'].unique().tolist()\n",
    "len(top_10_patients), len(set(top_10_patients).intersection(set(all_patients)))\n",
    "# df_top_10_patient_sentences_with_sleep = df_sentence_with_sleep[df_sentence_with_sleep.SUBJECT_ID.isin(top_10_patients)]\n",
    "# df_top_10_patient_sentences_without_sleep = df_sentence_without_sleep[df_sentence_without_sleep.SUBJECT_ID.isin(top_10_patients)]\n",
    "# df_not_top_10_patient_sentences = df_sentence_with_sleep[~df_sentence_with_sleep.SUBJECT_ID.isin(top_10_patients)]\n",
    "# print(\"top 10 patient sentences:\", len(df_top_10_patient_sentences_with_sleep))\n",
    "# print(\"top 10 patients sentences without sleep:\", len(df_top_10_patient_sentences_without_sleep))\n",
    "# print(\"other patient sentences:\", len(df_not_top_10_patient_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_10_patient_sentences_without_sleep.SUBJECT_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e561e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_isevent = df[df.SUBJECT_ID.isin(top_10_patients)].groupby(['ROW_ID','SUBJECT_ID'])['Event_Name'].agg(lambda x: any([ET in j for j in x])).reset_index()\n",
    "docid_isevent[docid_isevent.Event_Name==True]['SUBJECT_ID'].value_counts()\n",
    "res = docid_isevent.groupby([\"Event_Name\", \"SUBJECT_ID\"]).size().unstack(fill_value=0)\n",
    "res, res.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentid_isevent = df[df.SUBJECT_ID.isin(top_10_patients)].groupby(['UID','SUBJECT_ID'])['Event_Name'].agg(lambda x: any([ET in j for j in x])).reset_index()\n",
    "sentid_isevent[sentid_isevent.Event_Name==True]['SUBJECT_ID'].value_counts()\n",
    "res = sentid_isevent.groupby([\"Event_Name\", \"SUBJECT_ID\"]).size().unstack(fill_value=0)\n",
    "res, res.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N2 in range(7):\n",
    "    res = df_sentence_without_sleep[df_sentence_without_sleep.SUBJECT_ID.isin(top_10_patients[:N2])]\n",
    "    print(N2,len(res))\n",
    "    \n",
    "2366 + 2502, 121 + 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SUBJECT_ID.isin(top_10_patients)].ROW_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443daea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2355 + 147"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9485b",
   "metadata": {},
   "source": [
    "df.LOS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
