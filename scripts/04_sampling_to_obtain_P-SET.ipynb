{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385ae15b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../exports/selected_reports_with_event_log_only_dictionary_v2/combined.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m             df[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_similarity\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mSimilarity\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[ET])\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m df_dictionary = prepare_df(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../exports/selected_reports_with_event_log_only_dictionary_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/combined.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mdictionary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# df_biolord = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_biolord_v{version}/combined.pkl\"), type = \"biolord\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_text_to_event/.venv/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[39m, in \u001b[36mread_pickle\u001b[39m\u001b[34m(filepath_or_buffer, compression, storage_options)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m excs_to_catch = (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_text_to_event/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../exports/selected_reports_with_event_log_only_dictionary_v2/combined.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from config import event_types\n",
    "random.seed(42)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# jupyter nbconvert --to script 04b_combine_results_find_disagreement.ipynb\n",
    "# sudo kill -9 $(nvidia-smi | awk 'NR>8 {print $5}' | grep -E '^[0-9]+$')\n",
    "version = 2\n",
    "from config import event_types\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "def prepare_df(df, type=\"biolord\"):\n",
    "    global event_types\n",
    "    df = df.copy()\n",
    "    print(\"Loaded file len: \",len(df))\n",
    "    df['Sent_ID'] = df['Events'].apply(lambda x: [f\"{i:04d}\" for i in range(len(x))])\n",
    "    df = df.explode([\"Sent_ID\",\"Events\"])\n",
    "    print(\"After exploding file len: \",len(df))\n",
    "    df['UID'] = df['SUBJECT_ID'].astype(str) + \"_\" + df['ROW_ID'].astype(str) + \"_\" + df['Sent_ID'].astype(str)\n",
    "    df = df.dropna(subset=\"Events\")\n",
    "    df['Event_Name'] = df['Events'].apply(lambda x: x['event'])\n",
    "    df['Sentence'] = df['Events'].apply(lambda x: x['text'])\n",
    "    df['Time'] = df['Events'].apply(lambda x: x['event_detection_time'])\n",
    "    \n",
    "    if type == \"dictionary\":\n",
    "        df['Keyword'] = df['Events'].apply(lambda x: x['keyword'])\n",
    "        df['Lemma'] = df['Events'].apply(lambda x: x['lemma'])\n",
    "        df['Keyword_Position'] = df['Events'].apply(lambda x: x['keyword_position'])\n",
    "        \n",
    "    if type == \"biolord\":\n",
    "        df['Similarity'] = df['Events'].apply(lambda x: x['similarity'])\n",
    "        df['Similarity'] = df['Similarity'].apply(lambda x: {k:v for (k,v) in x.items() if k!=\"Alert And Oriented\"})\n",
    "        for ET in event_types:\n",
    "            df[f\"{ET}_similarity\"] = df['Similarity'].apply(lambda x:x[ET])\n",
    "            \n",
    "    return df\n",
    "df_dictionary = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_dictionary_v{version}/combined.pkl\"),type=\"dictionary\")\n",
    "# df_biolord = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_biolord_v{version}/combined.pkl\"), type = \"biolord\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccd116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
       "       'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'DOCUMENT',\n",
       "       'AGE', 'LOS_DAYS', 'IS_ALIVE', 'DOCUMENT_LOWER', 'NUM_NOTES',\n",
       "       'MAX_NOTES_PER_DAY', 'MAX_NOTES_PER_CHARTTIME', 'all_dates_present',\n",
       "       'Sentences', 'Sentences_Raw', 'Sentences_Cleaned', 'Events', 'Sent_ID',\n",
       "       'UID', 'Event_Name', 'Sentence', 'Time', 'Keyword', 'Lemma',\n",
       "       'Keyword_Position'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dictionary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary_exploded = df_dictionary.explode(['Keyword','Lemma','Event_Name','Keyword_Position'])\n",
    "df_dictionary_exploded['KUID'] = df_dictionary_exploded['UID'] + \"_\" + df_dictionary_exploded['Keyword_Position'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac88677",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_dictionary_exploded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_dictionary_exploded\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_dictionary_exploded' is not defined"
     ]
    }
   ],
   "source": [
    "df_dictionary_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fd6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Pain*** Pain counts:422\n",
      " Unique Sentences: 5620\n",
      " Unique Reports: 225\n",
      " Unique Patients: 10\n",
      "\n",
      "***Sleep*** Sleep counts:164\n",
      " Unique Sentences: 4783\n",
      " Unique Reports: 210\n",
      " Unique Patients: 10\n",
      "\n",
      "***Excretion*** Excretion counts:229\n",
      " Unique Sentences: 7382\n",
      " Unique Reports: 317\n",
      " Unique Patients: 10\n",
      "\n",
      "***Eating*** Eating counts:152\n",
      " Unique Sentences: 7267\n",
      " Unique Reports: 329\n",
      " Unique Patients: 10\n",
      "\n",
      "***Family*** Family counts:536\n",
      " Unique Sentences: 7910\n",
      " Unique Reports: 328\n",
      " Unique Patients: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "from config import event_types\n",
    "import os\n",
    "os.makedirs(\"../exports/groundtruth/P-SET/Generated\", exist_ok=True)\n",
    "for ET in event_types:\n",
    "    top_ET = df_dictionary_exploded[df_dictionary_exploded.Event_Name==ET][[\"SUBJECT_ID\",\"Event_Name\"]].value_counts().reset_index().iloc[:N]\n",
    "    selected_patient_sentences = df_dictionary[df_dictionary.SUBJECT_ID.isin(top_ET['SUBJECT_ID'].tolist())]\n",
    "    total_ET = top_ET[top_ET.Event_Name==ET]['count'].sum()\n",
    "    num_sentences = len(selected_patient_sentences.UID.unique())\n",
    "    num_reports = len(selected_patient_sentences.ROW_ID.unique())\n",
    "    num_patients = len(selected_patient_sentences.SUBJECT_ID.unique())\n",
    "    print(f\"***{ET}***\",\n",
    "          f\"{ET} counts:{total_ET}\\n\",\n",
    "        # f\"Event counts:\\n{selected_patient_sentences.Event_Name.value_counts()}\\n\"\n",
    "        f\"Unique Sentences: {num_sentences}\\n\",\n",
    "        f\"Unique Reports: {num_reports}\\n\",\n",
    "        f\"Unique Patients: {num_patients}\\n\"\n",
    "    )\n",
    "    selected_patient_sentences.to_pickle(f\"../exports/groundtruth/P-SET/Generated/{ET}_{num_patients}_{num_reports}_{num_sentences}_{total_ET}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d6d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c4cbbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1135_1277059_0000', '1135_1277059_0001', '1135_1277059_0002', ...,\n",
       "       '32291_1671540_0018', '32291_1671540_0019', '32291_1671540_0020'],\n",
       "      shape=(4783,), dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_patient_sentences.UID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbc26671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4783 4435 210 10 Event_Name\n",
      "[Unknown]                                  4311\n",
      "[Sleep]                                     114\n",
      "[Pain]                                      112\n",
      "[Family]                                    101\n",
      "[Excretion]                                  66\n",
      "[Eating]                                     21\n",
      "[Sleep, Sleep]                               16\n",
      "[Pain, Pain]                                 16\n",
      "[Pain, Sleep]                                 7\n",
      "[Family, Sleep]                               4\n",
      "[Eating, Eating]                              3\n",
      "[Family, Pain]                                2\n",
      "[Family, Family]                              2\n",
      "[Family, Family, Family, Family]              1\n",
      "[Family, Pain, Sleep, Sleep]                  1\n",
      "[Sleep, Sleep, Sleep]                         1\n",
      "[Eating, Family, Sleep]                       1\n",
      "[Excretion, Excretion]                        1\n",
      "[Family, Family, Family, Family, Sleep]       1\n",
      "[Family, Family, Family]                      1\n",
      "[Pain, Pain, Pain]                            1\n",
      "Name: count, dtype: int64\n",
      "Event_Name\n",
      "Unknown      4311\n",
      "Sleep         164\n",
      "Pain          157\n",
      "Family        124\n",
      "Excretion      68\n",
      "Eating         28\n",
      "Name: count, dtype: int64 Lemma\n",
      "sleep       93\n",
      "sleeping    42\n",
      "asleep      16\n",
      "nap         13\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SUBJECT_ID    15024.000000\n",
       "LOS_DAYS          9.918497\n",
       "AGE              62.960861\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "ET = \"Sleep\"\n",
    "df = pd.read_pickle(glob(f\"../exports/groundtruth/P-SET/Generated/{ET}*.pkl\")[0])\n",
    "print(df.UID.nunique(), df.Sentence.nunique(), df.ROW_ID.nunique(), df.SUBJECT_ID.nunique(), df.Event_Name.value_counts())\n",
    "\n",
    "df_exploded = df.explode(['Keyword','Lemma','Event_Name','Keyword_Position'])\n",
    "df_exploded['KUID'] = df_exploded['UID'] + \"_\" + df_exploded['Keyword_Position'].astype(str)\n",
    "print(df_exploded.Event_Name.value_counts(),df_exploded[df_exploded.Event_Name==ET]['Lemma'].value_counts())\n",
    "\n",
    "df[['SUBJECT_ID', 'LOS_DAYS', 'AGE']].drop_duplicates().mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9485b",
   "metadata": {},
   "source": [
    "df.LOS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
