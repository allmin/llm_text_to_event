{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53856441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file len:  15222\n",
      "After exploding file len:  338395\n",
      "Loaded file len:  15222\n",
      "After exploding file len:  338395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from config import event_types\n",
    "random.seed(42)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# jupyter nbconvert --to script 04b_combine_results_find_disagreement.ipynb\n",
    "# sudo kill -9 $(nvidia-smi | awk 'NR>8 {print $5}' | grep -E '^[0-9]+$')\n",
    "version = 1\n",
    "from config import event_types\n",
    "\n",
    "for model_type in [\"dictionary\", \"biolord\"]:\n",
    "    output_folder = f\"../exports/selected_reports_with_event_log_only_{model_type}_v{version}\"\n",
    "    output_file = f\"{output_folder}/combined.pkl\"\n",
    "    batch_files = sorted(glob.glob(f\"{output_folder}/batch_*.pkl\"))\n",
    "    combined_df = pd.concat([pd.read_pickle(f) for f in batch_files], ignore_index=True)\n",
    "    combined_df.to_pickle(output_file)\n",
    "   \n",
    "\n",
    "def prepare_df(df, type=\"biolord\"):\n",
    "    global event_types\n",
    "    df = df.copy()\n",
    "    print(\"Loaded file len: \",len(df))\n",
    "    df['Sent_ID'] = df['Events'].apply(lambda x: [f\"{i:04d}\" for i in range(len(x))])\n",
    "    df = df.explode([\"Sent_ID\",\"Events\"])\n",
    "    print(\"After exploding file len: \",len(df))\n",
    "    df['UID'] = df['ROW_ID'].astype(str) + \"_\" + df['Sent_ID'].astype(str)\n",
    "    df = df.dropna(subset=\"Events\")\n",
    "    df['Event_Name'] = df['Events'].apply(lambda x: x['event'])\n",
    "    df['Sentence'] = df['Events'].apply(lambda x: x['sentence'])\n",
    "    df['Time'] = df['Events'].apply(lambda x: x['event_detection_time'])\n",
    "    \n",
    "    if type == \"dictionary\":\n",
    "        df['Keyword'] = df['Events'].apply(lambda x: x['keyword'])\n",
    "        df['Lemma'] = df['Events'].apply(lambda x: x['lemma'])\n",
    "    if type == \"biolord\":\n",
    "        df['Similarity'] = df['Events'].apply(lambda x: x['similarity'])\n",
    "        df['Similarity'] = df['Similarity'].apply(lambda x: {k:v for (k,v) in x.items() if k!=\"Alert And Oriented\"})\n",
    "        for ET in event_types:\n",
    "            df[f\"{ET}_similarity\"] = df['Similarity'].apply(lambda x:x[ET])\n",
    "            \n",
    "    return df\n",
    "df_dictionary = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_dictionary_v{version}/combined.pkl\"),type=\"dictionary\")\n",
    "df_biolord = prepare_df(pd.read_pickle(f\"../exports/selected_reports_with_event_log_only_biolord_v{version}/combined.pkl\"), type = \"biolord\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a13cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_biolord.Similarity = df_biolord.Similarity.astype(str)\n",
    "similarity_col_names = [f\"{ET}_similarity\" for ET in event_types]\n",
    "df_both = pd.merge(df_dictionary[['UID','ROW_ID','Sent_ID','HADM_ID','CHARTTIME','STORETIME','Sentence','Event_Name','Keyword','Lemma','CGID','Time']], \n",
    "         df_biolord[['UID','ROW_ID','Sent_ID','HADM_ID','CHARTTIME','STORETIME','Sentence','Event_Name','CGID','Similarity','Time']+similarity_col_names], \n",
    "         on=['UID','HADM_ID','ROW_ID','Sent_ID'], how='outer',suffixes=(\"_dictionary\",\"_biolord\")).sort_values(by=['HADM_ID','ROW_ID','Sent_ID'])\n",
    "df_both[\"Event_Name_dictionary_multi\"] = df_both[\"Event_Name_dictionary\"].apply(lambda x: {i: x.count(i) if isinstance(x, list) else 0 for i in event_types})\n",
    "len(df_dictionary), len(df_biolord), len(df_both)\n",
    "df_both.to_pickle(\"../exports/dictionary_biolord_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Define a custom color gradient (e.g., black → red → yellow → white)\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    name='custom_gradient',\n",
    "    colors=[\"skyblue\", \"yellow\"]\n",
    ")\n",
    "def plot_axis(counts, i, ax, column, num_bins, bin_edges):\n",
    "\n",
    "    norm_counts = counts / counts.sum() * 100\n",
    "\n",
    "    sns.heatmap(\n",
    "    [norm_counts],\n",
    "    ax=ax,\n",
    "    cmap=custom_cmap,\n",
    "    cbar=False,\n",
    "    xticklabels=False if i < len(similarity_columns) - 1 else True,\n",
    "    yticklabels=False,\n",
    "    annot=[norm_counts],                 # Annotations need to match the shape of data (1 row)\n",
    "    fmt='0.1f',                        # Integer format\n",
    "    annot_kws={\"size\": 6, \"color\": \"black\", \"rotation\": 90, \"va\": \"center\", \"ha\": \"center\"}  # Styling\n",
    ")\n",
    "    if i%2==0:\n",
    "        pref = \"W\"\n",
    "    else:\n",
    "        pref = \"W/o\"\n",
    "    # Label each heatmap\n",
    "    ax.set_ylabel(f\"{pref} {column} KW \\n Total:{counts.sum()}\", rotation=0, labelpad=50, va='center')\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Only format x-ticks for the bottom axis\n",
    "    if i == 9:\n",
    "        # Compute bin centers\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "        # Choose a few bin positions to label\n",
    "        tick_indices = np.linspace(0, num_bins - 1, 25).astype(int)\n",
    "        tick_labels = [f\"{bin_centers[j]:.2f}\" for j in tick_indices]\n",
    "\n",
    "        # Now set ticks and labels\n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels, rotation=90, ha=\"right\")\n",
    "\n",
    "# Similarity columns to plot\n",
    "similarity_columns = event_types\n",
    "\n",
    "# Compute global min and max for consistent binning\n",
    "all_values = np.concatenate([df_both[f\"{col}_similarity\"].dropna().values for col in similarity_columns])\n",
    "global_min, global_max = all_values.min(), all_values.max()\n",
    "\n",
    "# Define consistent bins\n",
    "num_bins = 50\n",
    "bin_edges = np.linspace(global_min, global_max, num_bins + 1)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(similarity_columns)*2,\n",
    "    figsize=(12, 8),\n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    column = similarity_columns[i//2]\n",
    "    event_type = column\n",
    "    # if i%2 == 0:\n",
    "    #     df_sel = df_both[df_both[\"Event_Name_dictionary\"].apply(lambda x: \"_\".join(set(x)) == event_type )]\n",
    "    # else:\n",
    "    #     df_sel = df_both[df_both[\"Event_Name_dictionary\"].apply(lambda x: event_type not in x and \"_\".join(set(x))!='Unknown' )]\n",
    "    if i%2 == 0:\n",
    "        df_sel = df_both[df_both[\"Event_Name_dictionary\"].apply(lambda x: event_type in x )]\n",
    "    else:\n",
    "        df_sel = df_both[df_both[\"Event_Name_dictionary\"].apply(lambda x: event_type not in x and \"_\".join(set(x))!='Unknown' )]\n",
    "    counts, _ = np.histogram(df_sel[f\"{event_type}_similarity\"].dropna(), bins=bin_edges)\n",
    "\n",
    "    plot_axis(counts, i, ax, column, num_bins, bin_edges)\n",
    "\n",
    "# Add title and x label\n",
    "axes[0].set_title(\"1D Heatmap Histograms of Similarities\")\n",
    "axes[-1].set_xlabel(\"Similarities with Corresponding Event Types\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_pd_curves(df, cat='Lemma', meas=None, label=\"\"):\n",
    "    return None\n",
    "\n",
    "    # Prepare unique lemmas\n",
    "    cats = df[cat].value_counts().reset_index()[cat][:20]\n",
    "    print(df[cat].value_counts())\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Optional: custom color palette\n",
    "    palette = sns.color_palette(\"husl\", len(cats))\n",
    "\n",
    "    # Plot individual lemma distributions\n",
    "    for i, lemma in enumerate(cats):\n",
    "        subset = df[df[cat] == lemma]\n",
    "        sns.kdeplot(\n",
    "            subset[meas].dropna(),\n",
    "            bw_adjust=1,\n",
    "            label=lemma,\n",
    "            color=palette[i],\n",
    "            fill=False\n",
    "        )\n",
    "\n",
    "    # 🔁 Plot combined curve (all cats together)\n",
    "    # sns.kdeplot(\n",
    "    #     df[meas].dropna(),\n",
    "    #     bw_adjust=1,\n",
    "    #     label=f\"All {cat}s\",\n",
    "    #     color=\"black\",\n",
    "    #     linestyle=\"--\",\n",
    "    #     linewidth=2,\n",
    "    #     fill=False\n",
    "    # )\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.histplot(\n",
    "        df[meas].dropna(),\n",
    "        label=f\"All {cat}s\",\n",
    "        bins=15\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    plt.xlabel(meas)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{meas} Distributions by {cat}')\n",
    "    plt.legend(title=cat, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../exports/images/event_specific/{label}_event_type_{meas}.png\")\n",
    "    plt.show()\n",
    "    print(df[\"Lemma\"].value_counts())\n",
    "import os\n",
    "os.makedirs(\"../exports/images/event_specific/\", exist_ok=True)\n",
    "df_both_w_ET = {}\n",
    "df_both_wo_ET = {}\n",
    "# Explode and filter\n",
    "def weight_inverted_gaussian(x, k=10):\n",
    "    return 1 - np.exp(-k * (x - 0.5)**2)\n",
    "for col in ['Event_Name_dictionary', 'Keyword', 'Lemma']:\n",
    "    df_both[f\"{col}_bkp\"] = df_both[col].copy()\n",
    "     \n",
    "for ET in similarity_columns:\n",
    "    df_both_w_ET[ET] = df_both[df_both.Event_Name_dictionary.apply(lambda x: ET in x)].drop_duplicates(\"Sentence_dictionary\")\n",
    "    df_both_wo_ET[ET] = df_both[df_both.Event_Name_dictionary.apply(lambda x: ET not in x)].drop_duplicates(\"Sentence_dictionary\")\n",
    "\n",
    "    df_both_w_ET[ET] = df_both_w_ET[ET].explode(['Event_Name_dictionary', 'Keyword', 'Lemma'])\n",
    "    df_both_w_ET[ET] = df_both_w_ET[ET][df_both_w_ET[ET].Event_Name_dictionary==ET]\n",
    "    \n",
    "\n",
    "    df_both_wo_ET[ET] = df_both_wo_ET[ET].explode(['Event_Name_dictionary', 'Keyword', 'Lemma'])\n",
    "    df_both_wo_ET[ET]['Lemma'] = df_both_wo_ET[ET]['Lemma'].apply(lambda x: x if x!='' else \"<Unknown>\")\n",
    "\n",
    "\n",
    "    get_pd_curves(df_both_w_ET[ET],'Lemma',f'{ET}_similarity', label=\"With_Event_Before\")\n",
    "    get_pd_curves(df_both_wo_ET[ET],'Lemma',f'{ET}_similarity', label=\"Without_Event_Before\")\n",
    "\n",
    "\n",
    "\n",
    "    # Step 2: Add weight column\n",
    "    df_both_w_ET[ET] = df_both_w_ET[ET].copy()\n",
    "    df_both_w_ET[ET][\"sampling_weight\"] = df_both_w_ET[ET][f'{ET}_similarity'].apply(weight_inverted_gaussian)\n",
    "    df_both_wo_ET[ET] = df_both_wo_ET[ET].copy()\n",
    "    df_both_wo_ET[ET][\"sampling_weight\"] = df_both_wo_ET[ET][f'{ET}_similarity'].apply(weight_inverted_gaussian)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_both_w_ET[event_types[0]].Lemma.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "samples_per_bin_per_lemma_w_ET = 6\n",
    "samples_per_bin_per_lemma_wo_ET = 1\n",
    "samples_per_bin_per_unknown_lemma_w_ET = 6\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def bin_and_sample(df, similarity_col, lemma_col='Lemma', bins=5, samples_per_bin=6):\n",
    "    global samples_per_bin_per_unknown_lemma_w_ET\n",
    "    sampled_dfs = []\n",
    "    bin_edges = {}\n",
    "    for lemma, group in df.groupby(lemma_col):\n",
    "        group = group.copy()\n",
    "        # Compute consistent bins and store edges\n",
    "        bin_edges[lemma] = np.histogram_bin_edges(group[similarity_col], bins=bins)\n",
    "        # Use same edges in pd.cut\n",
    "        group['bin'] = pd.cut(group[similarity_col], bins=bin_edges[lemma], labels=False, include_lowest=True)\n",
    "        for b in range(len(bin_edges[lemma]) - 1):\n",
    "            bin_group = group[group['bin'] == b]\n",
    "            if not bin_group.empty:\n",
    "                if lemma == \"<Unknown>\":\n",
    "                    samples_per_bin=samples_per_bin_per_unknown_lemma_w_ET\n",
    "                sample_size = min(samples_per_bin, len(bin_group))\n",
    "                sampled = bin_group.sample(sample_size, random_state=42)\n",
    "                sampled_dfs.append(sampled)\n",
    "    sampled_df = pd.concat(sampled_dfs).drop(columns='bin')\n",
    "    return sampled_df\n",
    "\n",
    "# Step 3: Sample from each Lemma group\n",
    "for ET in similarity_columns:\n",
    "    kw_present_sampled_df = (\n",
    "    df_both_w_ET[ET]\n",
    "    .groupby(\"Lemma\", group_keys=False)\n",
    "    .apply(lambda g: bin_and_sample(\n",
    "        df=g, similarity_col=f\"{ET}_similarity\",\n",
    "        lemma_col = \"Lemma\", bins=5,\n",
    "        samples_per_bin=samples_per_bin_per_lemma_w_ET\n",
    "    )))\n",
    "    kw_not_present_sampled_df = (\n",
    "        df_both_wo_ET[ET]\n",
    "        .groupby(\"Lemma\", group_keys=False)\n",
    "        .apply(lambda g: bin_and_sample(\n",
    "            df=g, similarity_col=f\"{ET}_similarity\",\n",
    "            lemma_col = \"Lemma\", bins=5,\n",
    "            samples_per_bin=samples_per_bin_per_lemma_wo_ET\n",
    "    )))\n",
    "    \n",
    "    # get_pd_curves(kw_present_sampled_df,'Lemma',f'{ET}_similarity')\n",
    "    # get_pd_curves(kw_not_present_sampled_df,'Lemma',f'{ET}_similarity')\n",
    "\n",
    "    get_pd_curves(kw_present_sampled_df,'Lemma',f'{ET}_similarity', label=\"With_Event_After\")\n",
    "    get_pd_curves(kw_not_present_sampled_df,'Lemma',f'{ET}_similarity', label=\"Without_Event_After\")\n",
    "    import os\n",
    "    os.makedirs(\"../exports/groundtruth/F-SET/Generated\", exist_ok=True)\n",
    "    kw_present_sampled_df['is_keyword_present'] = True\n",
    "    \n",
    "    print(kw_present_sampled_df.Lemma.value_counts())\n",
    "    print(kw_not_present_sampled_df.Lemma.value_counts())\n",
    "    kw_not_present_sampled_df['is_keyword_present'] = False\n",
    "    combined_kw_present_not_present_sampled_df = pd.concat([kw_present_sampled_df,kw_not_present_sampled_df], ignore_index=True)\n",
    "    combined_kw_present_not_present_sampled_df[f\"gt_{ET}\"] = False\n",
    "    # assert(len(combined_kw_present_not_present_sampled_df.UID)==len(combined_kw_present_not_present_sampled_df.UID).unique())\n",
    "    combined_kw_present_not_present_sampled_df.to_pickle(f\"../exports/groundtruth/F-SET/Generated/{ET}_{len(kw_present_sampled_df)}_{len(kw_not_present_sampled_df)}.pkl\")\n",
    "    combined_kw_present_not_present_sampled_df.to_excel(f\"../exports/groundtruth/F-SET/Generated/{ET}_{len(kw_present_sampled_df)}_{len(kw_not_present_sampled_df)}.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ET in event_types:\n",
    "    display(ET, df_both.Event_Name_dictionary.apply(lambda x: ET in x).sum())\n",
    "    \n",
    "for ET in event_types:\n",
    "    display( len(df_both_w_ET[ET]), df_both_w_ET[ET].Lemma.value_counts().reset_index(), kw_present_sampled_df.Lemma.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
