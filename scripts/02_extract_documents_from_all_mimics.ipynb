{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08096dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ADMISSIONS.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m     dob = row[\u001b[33m'\u001b[39m\u001b[33mDOB\u001b[39m\u001b[33m'\u001b[39m].to_pydatetime()\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (admit - dob).days / \u001b[32m365.25\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m admissions = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ADMISSIONS.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m patients = pd.read_pickle(\u001b[33m\"\u001b[39m\u001b[33m../data/PATIENTS.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m date_cols = [\u001b[33m'\u001b[39m\u001b[33mDOB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDOD\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDOD_HOSP\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDOD_SSN\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_text_to_event/.venv/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[39m, in \u001b[36mread_pickle\u001b[39m\u001b[34m(filepath_or_buffer, compression, storage_options)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m excs_to_catch = (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_text_to_event/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/ADMISSIONS.pkl'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def calculate_age(row):\n",
    "    admit = row['ADMITTIME'].to_pydatetime()\n",
    "    dob = row['DOB'].to_pydatetime()\n",
    "    return (admit - dob).days / 365.25\n",
    "\n",
    "\n",
    "\n",
    "admissions = pd.read_pickle(\"../data/ADMISSIONS.pkl\")\n",
    "patients = pd.read_pickle(\"../data/PATIENTS.pkl\")\n",
    "date_cols = ['DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN']\n",
    "patients[date_cols] = patients[date_cols].apply(pd.to_datetime, errors='coerce')\n",
    "print(len(patients))\n",
    "alive_patients_list = patients[patients['DOD'].isna() & patients['DOD_HOSP'].isna() & patients['DOD_SSN'].isna() & (patients['EXPIRE_FLAG'] != 1)]['SUBJECT_ID'].tolist()\n",
    "print(len(alive_patients_list))\n",
    "admissions[\"LOS_DAYS\"] = (admissions[\"DISCHTIME\"] - admissions[\"ADMITTIME\"]).dt.total_seconds() / (24 * 3600)\n",
    "admissions.loc[:,'IS_ALIVE'] = admissions['SUBJECT_ID'].isin(alive_patients_list)\n",
    "subject_id_to_dob = {i:j for (i,j) in zip(patients[\"SUBJECT_ID\"], patients[\"DOB\"])}\n",
    "admissions['DOB'] = admissions['SUBJECT_ID'].map(subject_id_to_dob)\n",
    "admissions['AGE'] = admissions.apply(calculate_age, axis=1)\n",
    "\n",
    "print(admissions[['AGE','LOS_DAYS','IS_ALIVE']].describe())\n",
    "print(admissions['DIAGNOSIS'].value_counts().head(20))\n",
    "hadm_id_to_los_days = {i:j for (i,j) in zip(admissions[\"HADM_ID\"], admissions[\"LOS_DAYS\"])}\n",
    "hadm_id_to_age = {i:j for (i,j) in zip(admissions[\"HADM_ID\"], admissions[\"AGE\"])}\n",
    "hadm_to_is_alive = {i:j for (i,j) in zip(admissions[\"HADM_ID\"], admissions[\"IS_ALIVE\"])}\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of age at admission:\")\n",
    "admissions['AGE'].hist(bins=311)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of length of stay:\")\n",
    "admissions['LOS_DAYS'].hist(bins=100)\n",
    "plt.show()\n",
    "\n",
    "print(\"number of admissions that lasted 7 to 14 days:\", len(admissions[(admissions['LOS_DAYS'] >= 7) & (admissions['LOS_DAYS'] <= 14)]), 'of total admissions:', len(admissions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57340ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "min_perc = np.average([admissions['LOS_DAYS'].rank(ascending=True, method='average', pct=True, numeric_only=True)[i] for i in admissions[admissions['LOS_DAYS']==7].index])\n",
    "max_perc = np.average([admissions['LOS_DAYS'].rank(ascending=True, method='average', pct=True, numeric_only=True)[i] for i in admissions[admissions['LOS_DAYS']==14].index])\n",
    "print(f\"7 days correspond to percentile: {min_perc*100}, and 14 days correspond to {max_perc*100} percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing negative length of stay\n",
    "neg_los = admissions[(admissions.LOS_DAYS < 0)]\n",
    "neg_los_dead = neg_los[neg_los['IS_ALIVE'] == False]\n",
    "neg_los_alive = neg_los[neg_los['IS_ALIVE'] == True]\n",
    "print(neg_los['IS_ALIVE'].value_counts())\n",
    "print(neg_los_alive[['DIAGNOSIS','DISCHARGE_LOCATION']].value_counts())\n",
    "# conclusion: most of the negative length of stay is due to patients who died before being discharged, and the rest is due to patients who were discharged before being admitted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e501a",
   "metadata": {},
   "source": [
    "- Load_notes,\n",
    "- Remove ISERROR,\n",
    "- Select only Categories [\"Nursing\", \"Nursing/other\", \"Physician\", 'Discharge summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44908",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_pickle(\"../data/NOTEEVENTS.pkl\")\n",
    "print(notes.CATEGORY.value_counts())\n",
    "print(len(notes), len(notes.CATEGORY.unique()))\n",
    "selected_categories = [\"Nursing\", \"Nursing/other\"]\n",
    "notes = notes[notes['CATEGORY'].isin(selected_categories)]\n",
    "print(len(notes))\n",
    "notes = notes[notes.ISERROR!=1]\n",
    "print(len(notes))\n",
    "notes = notes[notes.TEXT!=\"\"]\n",
    "print(len(notes))\n",
    "\n",
    "notes_selected = notes.copy()\n",
    "notes_selected.loc[:,'AGE'] = notes_selected['HADM_ID'].map(hadm_id_to_age)\n",
    "notes_selected.loc[:,'LOS_DAYS'] = notes_selected['HADM_ID'].map(hadm_id_to_los_days)\n",
    "notes_selected.loc[:,'IS_ALIVE'] = notes_selected['HADM_ID'].map(hadm_to_is_alive)\n",
    "notes_selected.loc[:,'SUBJECT_ID'] = notes_selected['HADM_ID'].map(admissions.set_index('HADM_ID')['SUBJECT_ID'])\n",
    "print(f\"number of selected notes in {selected_categories}: {len(notes_selected)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15786930",
   "metadata": {},
   "source": [
    "\n",
    "Checking if changing report to lower case helps in number of reports to split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetition_stats(df,column):\n",
    "      row_frequency = df[column].value_counts().reset_index(name='frequency')\n",
    "      repeating_rows = row_frequency[row_frequency['frequency'] > 1]\n",
    "      unique_rows = row_frequency[row_frequency['frequency'] == 1]\n",
    "      print(f\"\"\"{len(repeating_rows)} rows repeat to make {repeating_rows[repeating_rows['frequency'] > 1]['frequency'].sum()} rows\\n\n",
    "            with an additional {len(unique_rows)} unique rows. This makes a total of {len(df)} rows.\"\"\")\n",
    "\n",
    "notes_selected.loc[:,'TEXT_LOWER'] = notes_selected.copy()['TEXT'].str.lower()\n",
    "print(\"TEXT\")\n",
    "get_repetition_stats(notes_selected,'TEXT')\n",
    "print(\"TEXT_LOWER\")\n",
    "get_repetition_stats(notes_selected,'TEXT_LOWER')\n",
    "\n",
    "print(f\"CONCLUSION: Just {61558-60253} reports get benefitted from lower case. but caching is can help 2 percent of the reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22544c34",
   "metadata": {},
   "source": [
    "- Remove error strings\n",
    "- Fill abbreviations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcec9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import spacy, importlib\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    \n",
    "import utils.nlp_tools as nlp_tools\n",
    "nlp_tools = importlib.reload(nlp_tools)\n",
    "# Load the English model (download if you haven't: python -m spacy download en_core_web_lg)\n",
    "nlp = nlp_tools.TextLib(\"en_core_web_lg\")\n",
    "\n",
    "from resources.abbreviations import abbreviation_dict\n",
    "\n",
    "report_counts = 0\n",
    "def replace_abbreviation_and_print_progress(text):\n",
    "    global abbreviation_dict, report_counts\n",
    "    report_counts += 1\n",
    "    if report_counts % 100000 == 0:\n",
    "        print(f\"Processed {report_counts} reports\")\n",
    "    if text is None:\n",
    "        return text\n",
    "    if isinstance(text, str):\n",
    "        text = nlp.replace_abbreviations(text, abbreviation_dict)\n",
    "    return text\n",
    "    \n",
    "print(\"Removing error strings\")\n",
    "notes_selected.loc[:,'TEXT'] = notes_selected['TEXT'].apply(lambda x: nlp.remove_error_strings(x))\n",
    "\n",
    "print(f\"Filling abbreviations on {len(notes_selected)} reports\")\n",
    "notes_selected.loc[:,'TEXT'] = notes_selected['TEXT'].apply(replace_abbreviation_and_print_progress)\n",
    "\n",
    "notes_selected.to_pickle(\"../data/NOTEEVENTS_NURSINGNOTES_REMOVED_ERROR_STRINGS_FILLED_ABBREVIATIONS_new.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4481f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_selected = pd.read_pickle(\"../data/NOTEEVENTS_NURSINGNOTES_REMOVED_ERROR_STRINGS_FILLED_ABBREVIATIONS_new.pkl\")\n",
    "hadm_to_num_notes = notes_selected.groupby('HADM_ID').size()\n",
    "notes_selected['CHARTDATE'] = pd.to_datetime(notes_selected['CHARTDATE'])\n",
    "max_notes_per_day = (notes_selected.groupby(['HADM_ID', 'CHARTDATE']).size().groupby(level='HADM_ID').max())\n",
    "maximum_notes_per_chart_time = (notes_selected.groupby(['HADM_ID', 'CHARTTIME']).size().groupby(level='HADM_ID').max())\n",
    "notes_selected['NUM_NOTES'] = notes_selected['HADM_ID'].map(hadm_to_num_notes)\n",
    "notes_selected['MAX_NOTES_PER_DAY'] = notes_selected['HADM_ID'].map(max_notes_per_day)\n",
    "notes_selected['MAX_NOTES_PER_CHARTTIME'] = notes_selected['HADM_ID'].map(maximum_notes_per_chart_time)\n",
    "\n",
    "def check_all_dates_present(group):\n",
    "    min_date = group['CHARTDATE'].min()\n",
    "    max_date = group['CHARTDATE'].max()\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    # Check if any dates are missing\n",
    "    all_present = all_dates.isin(group['CHARTDATE']).all()\n",
    "    return pd.Series({'all_dates_present': all_present})\n",
    "\n",
    "# Step 2: Apply the function to each HADM_ID\n",
    "date_check = notes_selected.groupby('HADM_ID').apply(check_all_dates_present).reset_index()\n",
    "\n",
    "# Step 3: Merge back to notes_selected\n",
    "notes_selected = notes_selected.merge(date_check, on='HADM_ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf696247",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_selected.all_dates_present.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c80d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quartiles(df, column):\n",
    "    \"\"\"\n",
    "    Calculate the quartiles (Q1, Q2, Q3) for a given column in a DataFrame and return the quartile values along with the subset of rows between Q1 and Q2.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The column name for which to calculate quartiles.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (q1, q2, q3, mid_df)\n",
    "            q1 (float): 25th percentile value.\n",
    "            q2 (float): Median (50th percentile) value.\n",
    "            q3 (float): 75th percentile value.\n",
    "            mid_df (pd.DataFrame): Subset of df where column values are between q1 and q2 (inclusive).\n",
    "    \"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q2 = df[column].median()\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    mid_df = df[(df[column]>=q1)&(df[column]<=q2)]\n",
    "    num = len(mid_df)\n",
    "    print(f\"{column} Quartiles :\")\n",
    "    print(f\"Q1 (25th percentile): {q1:.2f}\")\n",
    "    print(f\"Q2 (Median):          {q2:.2f}\")\n",
    "    print(f\"Q3 (75th percentile): {q3:.2f}\")\n",
    "    print(f\"Number of rows in this range q1-q3: {num}\")\n",
    "    return q1,q2,q3,mid_df\n",
    "\n",
    "# get_quartiles(notes_selected, 'AGE')\n",
    "get_quartiles(notes_selected, 'LOS_DAYS')\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of number of unique hadm_id per los_days:\")\n",
    "notes_selected[['HADM_ID','LOS_DAYS']].drop_duplicates()['LOS_DAYS'].hist(bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b0002",
   "metadata": {},
   "source": [
    "Filter\n",
    "- remove all reports with IS_ERROR\n",
    "- keep only reports from IS_ALIVE admissions\n",
    "- keep only Nursing/other and Nursing\n",
    "- Num reports must atleast be number of days of stay\n",
    "- people age of [18, 89) selected\n",
    "- Length of stay 7 to 14 days ( No reasoning yet)\n",
    "choose between:\n",
    "    - eliminate patients who have atleast two reports with same stortime.\n",
    "    - max num of reports per day <= 5 (too many reports due to show)\n",
    "    - min number of reports per day = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00449d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_days = 7\n",
    "max_days = 14\n",
    "# - remove empty TEXT\n",
    "\n",
    "# - remove all reports with IS_ERROR\n",
    "notes_selected = notes_selected[notes_selected['ISERROR'] != 1]\n",
    "# - keep only reports from IS_ALIVE admissions\n",
    "notes_selected = notes_selected[notes_selected['IS_ALIVE'] == True]\n",
    "# - keep only Nursing/other and Nursing # already performed. just to be sure\n",
    "notes_selected = notes_selected[notes_selected['CATEGORY'].isin([\"Nursing\", \"Nursing/other\"])] \n",
    "# - Num reports must atleast be number of days of stay\n",
    "notes_selected = notes_selected[notes_selected['NUM_NOTES'] >= notes_selected['LOS_DAYS']]\n",
    "# - people age of [18, 89) selected\n",
    "notes_selected = notes_selected[(notes_selected['AGE'] >= 18) & (notes_selected['AGE'] < 89)]\n",
    "# - Length of stay 5 to 14 days \n",
    "notes_selected = notes_selected[(notes_selected['LOS_DAYS'] >= min_days) & (notes_selected['LOS_DAYS'] <= max_days)]\n",
    "# - max num of reports per day <= 5 (too many reports due to show)\n",
    "notes_selected = notes_selected[notes_selected['MAX_NOTES_PER_DAY'] <= 5]\n",
    "# - all dates present\n",
    "notes_selected = notes_selected[notes_selected['all_dates_present'] == True]\n",
    "# - max num of reports per chart time == 1 (too many reports due to show)\n",
    "notes_selected = notes_selected[notes_selected['MAX_NOTES_PER_CHARTTIME'] == 1]\n",
    "len(notes_selected), len(notes_selected['HADM_ID'].unique()), len(notes_selected['SUBJECT_ID'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c45342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "os.makedirs(\"../exports/images\",exist_ok=True)\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of number of unique hadm_id per age:\")\n",
    "notes_selected[['HADM_ID','AGE']].drop_duplicates()['AGE'].hist(bins=89)\n",
    "plt.savefig(\"../exports/images/age_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of number of unique hadm_id per los_days:\")\n",
    "notes_selected[['HADM_ID','LOS_DAYS']].drop_duplicates()['LOS_DAYS'].hist(bins=10)\n",
    "plt.savefig(\"../exports/images/los_days_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b903fa",
   "metadata": {},
   "source": [
    "Extract sentences from the Report. and Clean the sentences as follows:\n",
    "\n",
    "- split sentences at ;\n",
    "- strinp blank leading and training white spaces\n",
    "- convert to lower case\n",
    "- strip off trailing periods\n",
    "- replace\"\\n\"->\"\"\n",
    "- replace double spaces with single spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a081bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text):\n",
    "    sentences_raw = nlp.sentence_splitter(text,span=False)\n",
    "    sentences = [f\"{sent['headers'][0]}: {sent['text']}\" if sent['headers'] else sent['text'] for sent in sentences_raw]\n",
    "    return sentences \n",
    "\n",
    "def clean_sentences(list_of_sentences):\n",
    "    result_list_of_sentences = []\n",
    "    for full_sentence in list_of_sentences:\n",
    "        splitted_sentences = full_sentence.split(\";\")\n",
    "        for sentence in splitted_sentences:\n",
    "            sentence = sentence.replace(\"\\n\",\"\").strip().lower().rstrip('.').replace(\"  \",\" \").replace(\"\\n\",\"\")\n",
    "            if sentence!= '':\n",
    "                result_list_of_sentences.append(sentence)\n",
    "    return result_list_of_sentences\n",
    "\n",
    "notes_selected['Sentences'] = [[]]*len(notes_selected)\n",
    "print(f\"Time to extract sentences from 100 reports: \")\n",
    "import time\n",
    "start_time = time.time()\n",
    "notes_selected.loc[:100,'Sentences'] = notes_selected['TEXT'].iloc[:100].apply(extract_sentences)\n",
    "end_time = time.time()\n",
    "time_per_100 = end_time - start_time\n",
    "print(f\"Time taken for 100 reports: {time_per_100} seconds\")\n",
    "print(f\" projected time for {len(notes_selected)} reports: {time_per_100 * (len(notes_selected) / 100) / 60} minutes\")\n",
    "print(\"extracting sentences for all reports...\")\n",
    "notes_selected.loc[:,'Sentences_Raw'] = notes_selected['TEXT'].apply(extract_sentences)\n",
    "notes_selected['Sentences_Cleaned'] = notes_selected['Sentences_Raw'].apply(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = f\"{min_days}_{max_days}_days\"\n",
    "notes_selected = notes_selected[notes_selected.Sentences_Cleaned.apply(lambda x: True if len(x)>0 else False)]\n",
    "notes_selected.columns = [i.replace('TEXT','DOCUMENT') for i in notes_selected.columns]\n",
    "notes_selected.to_pickle(f\"../exports/filtered_patient_reports_{suffix}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75176d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "suffix = f\"7_14_days\"\n",
    "notes_selected = pd.read_pickle(f\"../exports/filtered_patient_reports_{suffix}.pkl\")\n",
    "notes_selected[\"Sentences_length\"] = notes_selected['Sentences_Cleaned'].apply(lambda x: len(x))\n",
    "total_sentences = notes_selected['Sentences_length'].sum()\n",
    "print(f\"Total sentences extracted: {total_sentences}\")\n",
    "numberofreports = len(notes_selected)\n",
    "print(f\"Total number of reports: {numberofreports}\")\n",
    "sentences_per_report = notes_selected['Sentences_length'].mean()\n",
    "print(f\"Average number of sentences per report: {sentences_per_report}\")\n",
    "num_patients = notes_selected['HADM_ID'].nunique()\n",
    "print(f\"Total number of patients: {num_patients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9e903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
