{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092da256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools, os, sys\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import chi2_contingency\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from config import event_types\n",
    "disagreement_type = \"correct\" #| together, individual, anti \n",
    "def get_cohens_kappa(df,vis_columns,remove_multi=False):\n",
    "    cohenskappa_results = []\n",
    "\n",
    "    # Pairwise comparisons\n",
    "    for col1, col2 in itertools.combinations(vis_columns, 2):\n",
    "        sub_df = df.copy()\n",
    "        if remove_multi:\n",
    "            sub_df = sub_df[sub_df.apply(lambda x: True if (\"_\" not in x[col1] and '_' not in x[col2]) else False, axis=1)]\n",
    "        kappa = cohen_kappa_score(sub_df[col1],sub_df[col2])\n",
    "        cohenskappa_results.append({\n",
    "            'Column 1': col1,\n",
    "            'Column 2': col2,\n",
    "            'Cohen\\'s Kappa': kappa\n",
    "        })\n",
    "    cohenskappa_results_df = pd.DataFrame(cohenskappa_results)\n",
    "    return cohenskappa_results_df\n",
    "\n",
    "\n",
    "\n",
    "def get_chisquare(df, vis_columns):\n",
    "    chi2_results = []\n",
    "\n",
    "    for col1, col2 in itertools.combinations(vis_columns, 2):\n",
    "        # Drop NA values to avoid errors\n",
    "        sub_df = df[[col1, col2]].dropna()\n",
    "\n",
    "        # Create contingency table\n",
    "        contingency = pd.crosstab(sub_df[col1], sub_df[col2])\n",
    "\n",
    "        # Run Chi-Square test\n",
    "        chi2, p, _, _ = chi2_contingency(contingency)\n",
    "\n",
    "        chi2_results.append({\n",
    "            'Column 1': col1,\n",
    "            'Column 2': col2,\n",
    "            'Chi2 Stat': chi2,\n",
    "            'p-value': p\n",
    "        })\n",
    "\n",
    "    chi2_df = pd.DataFrame(chi2_results)\n",
    "    return chi2_df.sort_values(\"p-value\")\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrices_for_column_pairs(df, gt_col,vis_columns):\n",
    "\n",
    "    n_pairs = len(vis_columns)\n",
    "\n",
    "    # Determine layout: square-ish grid\n",
    "    n_cols = int(np.ceil(np.sqrt(n_pairs)))\n",
    "    n_rows = int(np.ceil(n_pairs / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    col1 = gt_col\n",
    "    for idx, col2 in enumerate(vis_columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Drop rows with missing values in either column\n",
    "        sub_df = df[[col1, col2]].dropna().copy()\n",
    "        start_length = len(sub_df)\n",
    "        # sub_df = sub_df[sub_df.apply(lambda x: True if (\"_\" not in x[col1] and '_' not in x[col2]) else False, axis=1)]\n",
    "        filter_length = len(sub_df)\n",
    "        # Get confusion matrix\n",
    "        labels = sorted(set(sub_df[col1]) | set(sub_df[col2]))\n",
    "        cm = confusion_matrix(sub_df[col1], sub_df[col2], labels=labels)\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "        ax.set_title(f\"{col1} vs \\n{col2}\\n strt: {start_length}\\nelim.:{filter_length-start_length}\", fontsize=10)\n",
    "        ax.set_xlabel(col2)\n",
    "        ax.set_ylabel(col1)\n",
    "\n",
    "    # Hide any extra axes\n",
    "    for j in range(len(vis_columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e84709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from IPython.display import display\n",
    "import random\n",
    "random.seed(42)\n",
    "from glob import glob\n",
    "# Load your data\n",
    "result_dict = {}\n",
    "\n",
    "def train_model(X,y):\n",
    "    results = []\n",
    "    f1s, accs, precs, recs = [], [], [], []\n",
    "    # Thresholds to test\n",
    "    thresholds = np.linspace(min(X), max(X), 100)\n",
    "    for threshold in thresholds:\n",
    "        preds = (X >= threshold).astype(int)\n",
    "        f1s.append(f1_score(y, preds))\n",
    "        accs.append(accuracy_score(y, preds))\n",
    "        precs.append(precision_score(y, preds, zero_division=0))\n",
    "        recs.append(recall_score(y, preds))\n",
    "        results.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"f1\": np.mean(f1s),\n",
    "            \"accuracy\": np.mean(accs),\n",
    "            \"precision\": np.mean(precs),\n",
    "            \"recall\": np.mean(recs)\n",
    "        })\n",
    "    # Convert results to DataFrame\n",
    "    plt.figure()\n",
    "    plt.plot(thresholds, f1s, label='F1')\n",
    "    plt.plot(thresholds, precs, label='Precision')\n",
    "    plt.plot(thresholds, recs, label='Recall')\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Find the best threshold by F1 (or any other metric)\n",
    "    best = results_df.loc[results_df[\"f1\"].idxmax()]\n",
    "    return best[\"threshold\"]\n",
    "\n",
    "\n",
    "def get_kfcv_results(X,y,label,K=5):\n",
    "    print(label)\n",
    "    print(f\"Number of True labels in y: {np.sum(y == 1)}\")\n",
    "    print(f\"Number of False labels in y: {np.sum(y == 0)}\")\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "    th_val, f1s_val, accs_val, precs_val, recs_val = [], [], [], [], []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        print(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}\")\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "\n",
    "        best_threshold = train_model(X_train,y_train)\n",
    "        X_val = X[val_idx]\n",
    "        y_val = y[val_idx]\n",
    "        preds = (X_val >= best_threshold).astype(int)\n",
    "        th_val.append(best_threshold)\n",
    "        f1s_val.append(f1_score(y_val, preds))\n",
    "        accs_val.append(accuracy_score(y_val, preds))\n",
    "        precs_val.append(precision_score(y_val, preds, zero_division=0))\n",
    "        recs_val.append(recall_score(y_val, preds))\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"threshold\": th_val,\n",
    "        \"f1\": f1s_val,\n",
    "        \"accuracy\": accs_val,\n",
    "        \"precision\": precs_val,\n",
    "        \"recall\": recs_val\n",
    "    })\n",
    "    \n",
    "    display(results_df)\n",
    "    print(\"Mean Metrics of K fold cross validation of Embedder\")\n",
    "    display(results_df.describe().loc['mean'])\n",
    "    best_th = results_df.describe().loc['mean']['threshold']\n",
    "    return best_th\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for ET in event_types:\n",
    "    print(ET)\n",
    "    if ET not in result_dict:\n",
    "        result_dict[ET] = {}\n",
    "    try:\n",
    "        file_path = glob(f\"../exports/groundtruth/F-SET/Annotated/{ET}*.xlsx\")[0]\n",
    "    except:\n",
    "        print(f\"No file found for {ET}, skipping...\")\n",
    "        continue\n",
    "    print(file_path)\n",
    "    df = pd.read_excel(file_path)  # or however you load it\n",
    "    print(df.columns)\n",
    "    print(\"Keyword Stats:\")\n",
    "    print(df.is_keyword_present.value_counts())\n",
    "    \n",
    "    \n",
    "    df = df.groupby(\"UID\")[[f\"{ET}_similarity\", f\"gt_{ET}\", \"is_keyword_present\", \"Sentence_dictionary\",\"Lemma\"]].agg(lambda x: max(x) if len(set(x))>1 else set(x).pop()).reset_index()\n",
    "    \n",
    "    print(\"After Grouping Sentences\")\n",
    "    print(df.is_keyword_present.value_counts())\n",
    "    df.dropna(subset=f\"gt_{ET}\",inplace=True)\n",
    "    print(\"After removing GT Unknown\")\n",
    "    print(df.is_keyword_present.value_counts())\n",
    "    print(len(df))\n",
    "    X = df[f\"{ET}_similarity\"].values\n",
    "    y = df[f\"gt_{ET}\"].values.astype(int)\n",
    "    y_dict = df.is_keyword_present.values\n",
    "\n",
    "    print(\"Metrics of dictionary\")\n",
    "    display({\"f1\":f1_score(y, y_dict), \"acc\":accuracy_score(y, y_dict), \n",
    "    \"prec\":precision_score(y, y_dict, zero_division=0), \"rec\":recall_score(y, y_dict.astype(int))})\n",
    "\n",
    "    X_all = X.copy()\n",
    "    y_all = y.copy()\n",
    "    \n",
    "    # th_all = train_model(X_all,y_all)\n",
    "    # preds = (X_all >= th_all).astype(int)\n",
    "    # print(\"th\",(th_all),\"f1\",(f1_score(y_all, preds)), \"acc\",accuracy_score(y_all, preds), \"prec\",precision_score(y_all, preds, zero_division=0),\"rec\", recall_score(y_all, preds))\n",
    "    \n",
    "    best_th_all = get_kfcv_results(X_all, y_all, label = \"*********All********\", K=5)\n",
    "    sentences_by_label = {\n",
    "        \"positive\": df.loc[(df[f\"gt_{ET}\"] == 1) & (df[f\"{ET}_similarity\"]<best_th_all), \"Sentence_dictionary\"].tolist(),\n",
    "        \"negative\": df.loc[(df[f\"gt_{ET}\"] == 0) & (df[f\"{ET}_similarity\"]>=best_th_all), \"Sentence_dictionary\"].tolist()\n",
    "        }\n",
    "    print(sentences_by_label)\n",
    "    tpr = df.groupby(['is_keyword_present','Lemma'])[f\"gt_{ET}\"].mean().reset_index()\n",
    "    keyword_present_tpr = tpr[tpr.is_keyword_present == 1]\n",
    "    keyword_absent_tpr = tpr[(tpr.is_keyword_present == 0) & (tpr[f\"gt_{ET}\"]>0)]\n",
    "    print(keyword_present_tpr, keyword_absent_tpr)\n",
    "    kw_mask = (df.is_keyword_present == True)\n",
    "    best_th_with = get_kfcv_results(X_all[kw_mask], y_all[kw_mask], label = \"*********With Keyword********\", K=3)\n",
    "    best_th_without = get_kfcv_results(X_all[~kw_mask], y_all[~kw_mask], label = \"*********Without Keyword********\", K=3)   \n",
    "\n",
    "    mixed_mask = ((X_all<best_th_with) | (X_all>=best_th_without))\n",
    "    agg_mask = (((X_all < best_th_without) & (~kw_mask) ) | ((X_all>=best_th_with) & (kw_mask)))\n",
    "    disagg_mask = (((X_all < best_th_with) & kw_mask ) | ((X_all>=best_th_without) & (~kw_mask)))\n",
    "\n",
    "    agg_mask = (((X_all < best_th_without) & (~kw_mask) ) | ((X_all>=best_th_with) & (kw_mask)))\n",
    "    plt.figure()\n",
    "    sns.histplot(X_all[kw_mask])\n",
    "    plt.axvline(x=best_th_with, color='red', linestyle='--', label=f'Best Th With ({best_th_with:.2f})')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    sns.histplot(X_all[~kw_mask])\n",
    "    plt.axvline(x=best_th_without, color='red', linestyle='--', label=f'Best Th Without ({best_th_without:.2f})')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    sns.histplot(X_all)\n",
    "    plt.axvline(x=best_th_with, color='red', linestyle='--', label=f'Best Th With ({best_th_with:.2f})')\n",
    "    plt.legend()\n",
    "    plt.axvline(x=best_th_without, color='red', linestyle='--', label=f'Best Th Without ({best_th_without:.2f})')\n",
    "    plt.legend()\n",
    "    \n",
    "    result_dict[ET]['th_all'] = best_th_all \n",
    "    result_dict[ET]['th_with'] = best_th_with\n",
    "    result_dict[ET]['th_without'] = best_th_without\n",
    "    result_dict[ET]['disagreement_mask'] = disagg_mask\n",
    "    result_dict[ET]['agreement_mask'] = agg_mask\n",
    "    result_dict[ET]['mixed_mask'] = mixed_mask\n",
    "    result_dict[ET]['disagreement_uid'] = df[disagg_mask][\"UID\"].tolist()  \n",
    "    result_dict[ET]['agreement_uid'] = df[agg_mask][\"UID\"].tolist()   \n",
    "    result_dict[ET]['mixed_uid'] = df[mixed_mask][\"UID\"].tolist()  \n",
    "    print(\"proportion of disagreement - dictionary, embedder\",result_dict[ET]['disagreement_mask'].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1363b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    " # \"agreement\" , \"disagreement\", \"mixed\", \"all\"\n",
    "\n",
    "def get_time(x):\n",
    "    x = np.array(x)\n",
    "    # Remove NaNs\n",
    "    x = x[~np.isnan(x)]\n",
    "    # If less than 3 values, just return mean\n",
    "    if len(x) < 3:\n",
    "        return np.mean(x)\n",
    "    # Remove outliers using IQR\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    mask = (x >= q1 - 1.5 * iqr) & (x <= q3 + 1.5 * iqr)\n",
    "    return np.mean(x[mask])\n",
    "\n",
    "def get_event(x):\n",
    "    global event_types\n",
    "    if x[\"event\"] != \"Unknown\":\n",
    "        return x[\"event\"]\n",
    "    x = x.copy()  # avoid modifying original\n",
    "    try:\n",
    "        raw = x.get(\"raw_output\", \"\")\n",
    "        if isinstance(raw, str):\n",
    "            parsed = json.loads(raw)\n",
    "            fields = parsed.keys()\n",
    "            common_to_labels = set(fields).intersection(set(event_types+[\"Unknown\"])) \n",
    "            if \"event_type\" in fields:\n",
    "                x[\"event\"] = parsed.get(\"event_type\")\n",
    "            elif \"event type\" in fields:\n",
    "                x[\"event\"] = parsed.get(\"event type\")\n",
    "            elif len(common_to_labels) > 0:\n",
    "                x[\"event\"] = list(common_to_labels)\n",
    "            else:\n",
    "                x[\"event\"] = \"Unknown\"\n",
    "        else:\n",
    "            x[\"event\"] = \"Unknown\"\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        x[\"event\"] = \"Unknown\"\n",
    "    return x[\"event\"]\n",
    "\n",
    "def fixnames(ls,suffix,remove):\n",
    "    LS=[]\n",
    "    for s in ls:\n",
    "        s = s.replace(\"_dict_evidence\",\"_Ki\").replace(\"_embedder_evidence\",\"_Si\").replace(\"_no_evidence\",\"\").replace(\"_all_evidence\",\"KiSi\").replace(\"Event_Name_\",'').replace(\"_Events\",'_').replace('_'+remove,\"\")\n",
    "        LS.append(s+suffix)\n",
    "    return LS\n",
    "\n",
    "def get_keyword(x, event_type):\n",
    "    x = x.copy()  # avoid modifying original\n",
    "    try:\n",
    "        raw = x.get(\"raw_output\", \"\")\n",
    "        if isinstance(raw, str):\n",
    "            parsed = json.loads(raw)\n",
    "            fields = parsed.keys()\n",
    "            common_to_labels = set(fields).intersection(set([\"Unknown\", \"Eating\", \"Sleep\", \"Excretion\", \"Family\", \"Pain\"])) \n",
    "            if \"keyword\" in fields:\n",
    "                x[\"keyword\"] = parsed.get(\"keyword\")\n",
    "                if type(x[\"keyword\"]) == dict:\n",
    "                    if event_type in x[\"keyword\"]:\n",
    "                        x[\"keyword\"] = x[\"keyword\"][event_type]\n",
    "                    else:\n",
    "                        x[\"keyword\"] = \"Unknown\"\n",
    "            elif \"keywords\" in fields:\n",
    "                x[\"keyword\"] = parsed.get(\"keywords\")\n",
    "                if type(x[\"keyword\"]) == dict:\n",
    "                    if event_type in x[\"keyword\"]:\n",
    "                        x[\"keyword\"] = x[\"keyword\"][event_type]\n",
    "                    else:\n",
    "                        x[\"keyword\"] = \"Unknown\"\n",
    "            elif event_type in fields:\n",
    "                x[\"keyword\"] = parsed.get(event_type)\n",
    "            else:\n",
    "                x[\"keyword\"] = \"Unknown\"\n",
    "        else:\n",
    "            x[\"keyword\"] = \"Unknown\"\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        x[\"keyword\"] = \"Unknown\"\n",
    "    return x[\"keyword\"]\n",
    "\n",
    "LLM_dict = {}\n",
    "df_both = pd.read_pickle(\"../exports/dictionary_biolord_features.pkl\")\n",
    "for analysis_type in [\"mixed\", \"agreement\", \"disagreement\", \"all\"]: #(M-SET, A-SET, D-SET, F-SET)\n",
    "    for ET in [\"Sleep\",\"Excretion\"]:\n",
    "        print(f\"********************{ET}************************\")\n",
    "        disag_uid = result_dict[ET]['disagreement_uid']\n",
    "        agg_uid = result_dict[ET]['agreement_uid']\n",
    "        mixed_uid = result_dict[ET]['mixed_uid']\n",
    "        llm_files = glob(f\"../exports/llm/{ET}/*.pkl\")\n",
    "        gt_file = glob(f\"../exports/groundtruth/F-SET/Annotated/{ET}*.xlsx\")[0]\n",
    "        gt_df = pd.read_excel(gt_file)\n",
    "        gt_df = gt_df.dropna(subset=f\"gt_{ET}\")\n",
    "        llm_models = ['LLM_Events_no_evidence','LLM_Events_dict_evidence','LLM_Events_embedder_evidence','LLM_Events_all_evidence']\n",
    "        # gt_df = gt_df.groupby(\"UID\")[f\"gt_{ET}\"].agg(lambda x: max(x)).reset_index()\n",
    "        gt_df = gt_df.groupby(\"UID\")[[f\"{ET}_similarity\", f\"gt_{ET}\", \"is_keyword_present\", \"Sentence_dictionary\",\"Lemma\",\"HADM_ID\"]].agg(lambda x: max(x) if len(set(x))>1 else set(x).pop()).reset_index()\n",
    "        uid_to_gt = {row['UID']:row[f\"gt_{ET}\"] for _,row in gt_df.iterrows()}\n",
    "        uid_to_key_present = {row['UID']:row[f\"is_keyword_present\"] for _,row in gt_df.iterrows()}\n",
    "        uid_to_lemma = {row['UID']:row[f\"Lemma\"] for _,row in gt_df.iterrows()}\n",
    "        uid_to_hadmid = {row['UID']:row[f\"HADM_ID\"] for _,row in gt_df.iterrows()}\n",
    "        uid_to_dict_time = {row['UID']:row[f\"Time_dictionary\"] for _,row in df_both.iterrows()}\n",
    "        output_folder = f\"../exports/disagreements/{ET}\"\n",
    "        os.makedirs(output_folder,exist_ok=True)\n",
    "        for file in llm_files:\n",
    "            filename = os.path.basename(file).rstrip('.pkl')\n",
    "            print(filename)\n",
    "            _,keyword_requested,_,phrase_requested = filename.split(\"_\")[-4:]\n",
    "            keyword_requested = eval(keyword_requested)\n",
    "            phrase_requested = eval(phrase_requested)\n",
    "            suffix = \"Ko\" if keyword_requested else \"\" \n",
    "            suffix = suffix + (\"Qo\" if phrase_requested else \"\")\n",
    "            \n",
    "            print(suffix,keyword_requested,phrase_requested,filename)\n",
    "            \n",
    "            df = pd.read_pickle(file)\n",
    "            df[f'dictionary_{ET}_time'] = df[\"UID\"].apply(lambda x:uid_to_dict_time.get(x))\n",
    "            if analysis_type == \"disagreement\":\n",
    "                df = df[df.UID.isin(disag_uid)]\n",
    "            elif analysis_type ==\"agreement\":\n",
    "                df = df[df.UID.isin(agg_uid)]\n",
    "            elif analysis_type ==\"mixed\":\n",
    "                df = df[df.UID.isin(mixed_uid)]\n",
    "            elif analysis_type == \"all\":\n",
    "                df=df.copy()\n",
    "            df[f\"{ET}_similarity\"] = df['Similarity'].apply(lambda x: x[ET])\n",
    "            df[\"Keyword_dictionary\"] = df[\"Keyword\"]\n",
    "            df[\"focus_event\"] = ET\n",
    "            df[f\"GT_{ET}\"] = df[\"UID\"].apply(lambda x:uid_to_gt.get(x))\n",
    "            df[f\"is_keyword_present\"] = df[\"UID\"].apply(lambda x:uid_to_key_present.get(x))\n",
    "            display(df.is_keyword_present.value_counts())\n",
    "            \n",
    "            print(df[[f\"GT_{ET}\",\"is_keyword_present\"]].value_counts())\n",
    "            \n",
    "            \n",
    "            df[f\"dict_Lemma\"] = df[\"UID\"].apply(lambda x:uid_to_lemma.get(x))\n",
    "            df[f\"HADM_ID\"] = df[\"UID\"].apply(lambda x:uid_to_lemma.get(x))\n",
    "            df.dropna(subset=f\"GT_{ET}\",inplace=True)\n",
    "            for col in llm_models:\n",
    "                df[f\"Event_Name_{col}\"] = df[col].apply(get_event)\n",
    "                df[f\"Keyword_{col}\"] = df[col].apply(lambda x:get_keyword(x,ET))\n",
    "                df[f\"{col}_{ET}_time\"] = df[col].apply(lambda x:x['event_detection_time'])\n",
    "            if len(df) > 1:\n",
    "                first_row = df.iloc[0]\n",
    "                splittable_columns = [\"Event_Name_dictionary\",'Event_Name_LLM_Events_no_evidence','Event_Name_LLM_Events_dict_evidence','Event_Name_LLM_Events_embedder_evidence','Event_Name_LLM_Events_all_evidence']\n",
    "                for model in llm_models:\n",
    "                    if phrase_requested:\n",
    "                        df[f'Phrase_{model}'] = df[model].apply(lambda x: x['phrase'])\n",
    "                disagreement_dfs = []\n",
    "                generated_columns = []\n",
    "\n",
    "                for col in splittable_columns:\n",
    "                    generated_column = f\"{col}_{ET}\"\n",
    "                    df[generated_column] = df[col].apply(lambda x: 1 if ET in x else 0)\n",
    "                    \n",
    "                    generated_columns.append(generated_column)\n",
    "                \n",
    "                f1s, accs, precs, recs, psup, nsup, times = [],[],[],[],[],[],[]\n",
    "                y_gt = df[f\"GT_{ET}\"]\n",
    "                LLM_dict[(keyword_requested,phrase_requested,f\"gt_{ET}\")] = y_gt\n",
    "                for col in generated_columns:\n",
    "                    preds = df[col]\n",
    "                    LLM_dict[(keyword_requested,phrase_requested,col)] = preds\n",
    "                    f1s.append(f1_score(y_gt, preds))\n",
    "                    accs.append(accuracy_score(y_gt, preds))\n",
    "                    precs.append(precision_score(y_gt, preds, zero_division=0))\n",
    "                    recs.append(recall_score(y_gt, preds))   \n",
    "                    psup.append(sum(y_gt))\n",
    "                    nsup.append(sum(y_gt==0))\n",
    "                    times.append(get_time(df[f\"{col.lstrip('Event_Name_')}_time\"]))\n",
    "                \n",
    "               \n",
    "                results_df = {\"technique\":fixnames(generated_columns,suffix,ET), \"pos_sup\": psup, \"neg_sup\": nsup, \"f1_score\":f1s, \"precision\":precs, \"recall\":recs, \"time\":times} \n",
    "                results = pd.DataFrame(results_df)   \n",
    "                plot_confusion_matrices_for_column_pairs(df, f\"GT_{ET}\",vis_columns=generated_columns)\n",
    "                op_path = f\"{output_folder}/{analysis_type}_{disagreement_type}_{filename}.xlsx\"\n",
    "                df['dict-embedder-status'] = \"\"\n",
    "                df[df[f\"Event_Name_dictionary_{ET}\"] & (df[f\"{ET}_similarity\"]>=best_th_with)]['dict-embedder-status'] = \"TT\"\n",
    "                df[df[f\"Event_Name_dictionary_{ET}\"] & (df[f\"{ET}_similarity\"]<best_th_with)]['dict-embedder-status'] = \"TF\"\n",
    "                df[~df[f\"Event_Name_dictionary_{ET}\"] & (df[f\"{ET}_similarity\"]<best_th_without)]['dict-embedder-status'] = \"FT\"\n",
    "                df[~df[f\"Event_Name_dictionary_{ET}\"] & (df[f\"{ET}_similarity\"]>=best_th_without)]['dict-embedder-status'] = \"FT\"\n",
    "\n",
    "\n",
    "                df.to_excel(op_path,index=False)\n",
    "                print(f\"file written to {op_path}\")\n",
    "                display(results)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890aa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import accuracy_score\n",
    "ET = \"Excretion\"\n",
    "# list_models = [(False, False, 'gt_Sleep'), (False, False, 'Event_Name_dictionary_Sleep'), (False, False, 'Event_Name_LLM_Events_no_evidence_Sleep'), (False, False, 'Event_Name_LLM_Events_dict_evidence_Sleep'), (False, False, 'Event_Name_LLM_Events_embedder_evidence_Sleep'), (False, False, 'Event_Name_LLM_Events_all_evidence_Sleep'), (False, True, 'gt_Sleep'), (False, True, 'Event_Name_dictionary_Sleep'), (False, True, 'Event_Name_LLM_Events_no_evidence_Sleep'), (False, True, 'Event_Name_LLM_Events_dict_evidence_Sleep'), (False, True, 'Event_Name_LLM_Events_embedder_evidence_Sleep'), (False, True, 'Event_Name_LLM_Events_all_evidence_Sleep'), (True, False, 'gt_Sleep'), (True, False, 'Event_Name_dictionary_Sleep'), (True, False, 'Event_Name_LLM_Events_no_evidence_Sleep'), (True, False, 'Event_Name_LLM_Events_dict_evidence_Sleep'), (True, False, 'Event_Name_LLM_Events_embedder_evidence_Sleep'), (True, False, 'Event_Name_LLM_Events_all_evidence_Sleep'), (True, True, 'gt_Sleep'), (True, True, 'Event_Name_dictionary_Sleep'), (True, True, 'Event_Name_LLM_Events_no_evidence_Sleep'), (True, True, 'Event_Name_LLM_Events_dict_evidence_Sleep'), (True, True, 'Event_Name_LLM_Events_embedder_evidence_Sleep'), (True, True, 'Event_Name_LLM_Events_all_evidence_Sleep')]\n",
    "list_models = [(False, False, f'gt_{ET}'), (False, False, f'Event_Name_dictionary_{ET}'), \n",
    "               (False, False, f'Event_Name_LLM_Events_no_evidence_{ET}'), (False, False, f'Event_Name_LLM_Events_dict_evidence_{ET}'), \n",
    "               (True, False, f'Event_Name_LLM_Events_no_evidence_{ET}'), \n",
    "               ]\n",
    "\n",
    "for i in range(len(list_models)):\n",
    "    for j in range(i + 1, len(list_models)):\n",
    "        model_A = list_models[i]\n",
    "        model_B = list_models[j]\n",
    "        y_true = LLM_dict[(False, False, f'gt_{ET}')]\n",
    "        y_pred_A = LLM_dict[model_A]\n",
    "        y_pred_B = LLM_dict[model_B]\n",
    "\n",
    "\n",
    "\n",
    "        correct_A = [a == t for a, t in zip(y_pred_A, y_true)]\n",
    "        correct_B = [b == t for b, t in zip(y_pred_B, y_true)]\n",
    "\n",
    "        b = sum((a == 1 and b == 0) for a, b in zip(correct_A, correct_B))\n",
    "        c = sum((a == 0 and b == 1) for a, b in zip(correct_A, correct_B))\n",
    "\n",
    "        table = [[0, b],\n",
    "                 [c, 0]]\n",
    "\n",
    "        result = mcnemar(table, exact=True)\n",
    "        print(f\"\\nComparing {model_A} vs {model_B}: \\n   Contigency Table: {table} \\n   p-value = {result.pvalue}\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "y_true = LLM_dict[(False, False, f'gt_{ET}')]\n",
    "\n",
    "for model in list_models:\n",
    "    y_pred = LLM_dict[model]\n",
    "    \n",
    "    # Precision, Recall, F1 for positive class (label 1)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    mcc= matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # PR-AUC (if predictions are binary, average_precision_score still works)\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall:    {recall:.4f}\")\n",
    "    print(f\"   F1 Score:  {f1:.4f}\")\n",
    "    print(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "    print(f\"   MCC:    {mcc:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in list_models:\n",
    "    y_pred = LLM_dict[model]\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    results.append((model, precision, recall, f1, pr_auc, mcc))\n",
    "\n",
    "# Sort by F1 or PR-AUC\n",
    "sorted_results = sorted(results, key=lambda x: x[5], reverse=True)  # sort by F1\n",
    "\n",
    "for model, prec, rec, f1, pr, mcc in sorted_results:\n",
    "    print(f\"{model} -> Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, PR-AUC: {pr:.3f}, MCC: {mcc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true), y_true.mean(), set(y_true), len(y_pred_A), y_pred_A.mean(), len(y_pred_B), y_pred_B.mean(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0020e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET=\"Sleep\"\n",
    "gt_df = pd.read_excel(f\"../exports/04_groundtruth/F-SET/Annotated/{ET}_155_233.xlsx\")\n",
    "df_both = pd.read_pickle(\"../exports/dictionary_biolord_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Counter\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "def get_lemma_data():\n",
    "    with open(\"../resources/lemma.en.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    lemma_data = []\n",
    "    for line in lines:\n",
    "        lemma_part, forms_part = line.strip().split(\" -> \")\n",
    "        lemma = lemma_part.split(\"/\")[0]  # Remove frequency\n",
    "        all_forms = forms_part.split(\",\") + [lemma]\n",
    "        lemma_data.append({\"lemma\": lemma, \"all_forms\": all_forms})\n",
    "    lemma_data = pd.DataFrame(lemma_data)\n",
    "    return lemma_data\n",
    "\n",
    "# Download required resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # for lemmatizer to work well\n",
    "\n",
    "lemma_data = get_lemma_data()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords list (only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stop words set for fast lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def unravel(x):\n",
    "    if \"{\" in x: \n",
    "        x= eval(x).values()  \n",
    "        if type(x) == tuple and type(x[0]) == dict:\n",
    "            x[0] = x[0].values()\n",
    "    return x\n",
    "\n",
    "def get_word_counts(x):\n",
    "    x = str(list(x) if type(x)!=str else x)\n",
    "    x = ''.join([ch for ch in x if ch.isalpha() or ch == ',' or ch.isspace()])\n",
    "    x = x.replace(',','')\n",
    "    x = x.replace('Unknown','')\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_lemma(token):\n",
    "    global lemma_data\n",
    "    searched_lemma = lemma_data[lemma_data[\"all_forms\"].apply(lambda x: token in x)]['lemma']\n",
    "    if len(searched_lemma) > 0:\n",
    "        return searched_lemma.iloc[0]\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "\n",
    "def value_count_words(x, n=2):\n",
    "    global lemma_data\n",
    "    ngrams = []\n",
    "\n",
    "    for sentence in x:\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            lemmas = [get_lemma(token) for token in tokens]\n",
    "            \n",
    "            # Build n-grams\n",
    "            if len(lemmas) >= n:\n",
    "                ngram_tuples = zip(*[lemmas[i:] for i in range(n)])\n",
    "                ngram_strings = [' '.join(gram) for gram in ngram_tuples]\n",
    "                ngrams.extend(ngram_strings)\n",
    "\n",
    "    return Counter(ngrams)\n",
    "\n",
    "\n",
    "for ET in event_types:\n",
    "    for dtype in [\"agreement\", \"disagreement\", \"all\", \"mixed\"]:\n",
    "        try:\n",
    "            dfs = pd.read_excel(glob(f\"../exports/disagreements/{ET}/{dtype}_correct_{ET}_*_kw_False_phrase_False.xlsx\")[0])\n",
    "        except:\n",
    "            print(f\"No file found for {ET} and {dtype}, skipping...\")\n",
    "            continue\n",
    "        dfgs = dfs.groupby([\"is_keyword_present\",f\"GT_{ET}\",'dict-embedder-status',f\"Event_Name_LLM_Events_no_evidence_{ET}\",f\"Event_Name_LLM_Events_dict_evidence_{ET}\"])['Sentence_dictionary'].agg(lambda x:list(x)).reset_index()\n",
    "        dfgs['count'] = dfgs.Sentence_dictionary.apply(len)\n",
    "        dfgs.to_excel(f\"../exports/paper/{ET}_{dtype}_llm_basic.xlsx\")\n",
    "\n",
    "        try:\n",
    "            dft = pd.read_excel(glob(f\"../exports/disagreements/{ET}/{dtype}_correct_{ET}_*_kw_True_phrase_False.xlsx\")[0])\n",
    "        except:\n",
    "            print(f\"No file found for {ET} and {dtype}, skipping...\")\n",
    "            continue\n",
    "        dft['base_llm_no_evidence'] = dfs[f'Event_Name_LLM_Events_no_evidence_{ET}']\n",
    "        dfgt = dft.groupby([\"is_keyword_present\",f\"GT_{ET}\",'dict-embedder-status',\"base_llm_no_evidence\",f\"Event_Name_LLM_Events_no_evidence_{ET}\"])[['Sentence_dictionary','Keyword_LLM_Events_no_evidence']].agg(lambda x:list(x)).reset_index()\n",
    "        dfgt['count'] = dfgt.Sentence_dictionary.apply(len)\n",
    "        dfgt.to_excel(f\"../exports/paper/{ET}_{dtype}_llm_basic_evidence.xlsx\")\n",
    "\n",
    "\n",
    "        dictionary = pd.read_excel(\"../resources/keyword_dict_annotated_expanded.xlsx\")\n",
    "        dft['keyword_best_llm'] = dft[\"Keyword_LLM_Events_no_evidence\"].apply(lambda x: unravel(x))\n",
    "        df_sel2 = dft[dft[f\"Event_Name_LLM_Events_no_evidence_{ET}\"]==1]\n",
    "\n",
    "        val_c_1 = value_count_words(df_sel2['keyword_best_llm'].apply(get_word_counts).tolist(),1)\n",
    "        val_c_1 = {k: v for k, v in sorted(val_c_1.items(), key=lambda item: item[1], reverse=True) if k.lower() not in stop_words}\n",
    "        val_c_1 = {k:v for (k,v) in val_c_1.items() if k not in dictionary['form'].tolist()}\n",
    "\n",
    "        val_c_2 = value_count_words(df_sel2['keyword_best_llm'].apply(get_word_counts).tolist(),2)\n",
    "        val_c_2 = {k: v for k, v in sorted(val_c_2.items(), key=lambda item: item[1], reverse=True) if k.lower() not in stop_words}\n",
    "        val_c_2 = {k:v for (k,v) in val_c_2.items() if all([jk not in dictionary['form'].tolist() for jk in k.split(' ')]) and not any([i in stop_words for i in k.split(' ')])}\n",
    "\n",
    "        val_c_3 = value_count_words(df_sel2['keyword_best_llm'].apply(get_word_counts).tolist(),3)\n",
    "        val_c_3 = {k: v for k, v in sorted(val_c_3.items(), key=lambda item: item[1], reverse=True) if k.lower() not in stop_words}\n",
    "        val_c_3 = {k:v for (k,v) in val_c_3.items() if all([jk not in dictionary['form'].tolist() for jk in k.split(' ')]) and not any([i in stop_words for i in k.split(' ')])}\n",
    "\n",
    "        val_c_4 = value_count_words(df_sel2['keyword_best_llm'].apply(get_word_counts).tolist(),4)\n",
    "        val_c_4 = {k: v for k, v in sorted(val_c_4.items(), key=lambda item: item[1], reverse=True) if k.lower() not in stop_words}\n",
    "        val_c_4 = {k:v for (k,v) in val_c_4.items() if all([jk not in dictionary['form'].tolist() for jk in k.split(' ')]) and not any([i in stop_words for i in k.split(' ')])}\n",
    "\n",
    "        print(val_c_1)\n",
    "        print(val_c_2)\n",
    "        print(val_c_3)\n",
    "        print(val_c_4)\n",
    "\n",
    "\n",
    "\n",
    "        combined_dicts = {\n",
    "            \"ngram_1\": val_c_1,\n",
    "            \"ngram_2\": val_c_2,\n",
    "            \"ngram_3\": val_c_3,\n",
    "            \"ngram_4\": val_c_4\n",
    "        }\n",
    "\n",
    "        os.makedirs(\"../exports/keywords_extracted\", exist_ok=True)\n",
    "\n",
    "        # Write to file\n",
    "        with open(f\"../exports/keywords_extracted/{ET}_{dtype}.json\", \"w\") as f:\n",
    "            json.dump(combined_dicts, f, indent=4)\n",
    "\n",
    "        print(f\"../exports/keywords_extracted/{ET}_{dtype}.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
